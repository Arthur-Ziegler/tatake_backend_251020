"""
èŠå¤©å›¾å®šä¹‰

åŸºäºLangGraphæ„å»ºèŠå¤©å¯¹è¯å›¾ï¼Œå®šä¹‰å¯¹è¯æµç¨‹å’ŒèŠ‚ç‚¹é€»è¾‘ã€‚
å®ç°ç®€å•çš„å¯¹è¯æ¨¡å¼ï¼ŒåŒ…å«agentèŠ‚ç‚¹å’Œå·¥å…·èŠ‚ç‚¹ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨ã€‚

è®¾è®¡åŸåˆ™ï¼š
1. ç®€å•ç›´æ¥çš„å¯¹è¯æµç¨‹
2. æ¸…æ™°çš„èŠ‚ç‚¹èŒè´£åˆ†ç¦»
3. çµæ´»çš„å·¥å…·é›†æˆ
4. å®Œæ•´çš„é”™è¯¯å¤„ç†

å›¾ç»“æ„ï¼š
START â†’ agent â†’ [æ¡ä»¶è·¯ç”±] â†’ {tools, END}
tools â†’ agent â†’ [æ¡ä»¶è·¯ç”±] â†’ {tools, END}

åŠŸèƒ½ç‰¹æ€§ï¼š
- å¯¹è¯çŠ¶æ€ç®¡ç†
- å·¥å…·è°ƒç”¨é›†æˆ
- æ¡ä»¶è·¯ç”±é€»è¾‘
- æ¶ˆæ¯å¤„ç†æµç¨‹

ä½œè€…ï¼šTaKeKeå›¢é˜Ÿ
ç‰ˆæœ¬ï¼š1.0.0
"""

import os
import logging
from typing import Dict, Any, Literal
from datetime import datetime

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.store.memory import InMemoryStore
from langgraph.prebuilt import ToolNode

from .models import ChatState
from .tools.password_opener import sesame_opener
from .tools.task_query import query_tasks, get_task_detail
from .tools.task_crud import create_task, update_task, delete_task
from .tools.task_search import search_tasks
from .tools.task_batch import batch_create_subtasks
from .prompts.system import format_system_prompt
from .context_manager import manage_conversation_context

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class ChatGraph:
    """
    èŠå¤©å›¾ç±»

    å°è£…LangGraphå›¾çš„æ„å»ºå’Œç¼–è¯‘é€»è¾‘ï¼Œæä¾›ç»Ÿä¸€çš„èŠå¤©å¯¹è¯æ¥å£ã€‚
    """

    def __init__(self, checkpointer: SqliteSaver, store: InMemoryStore):
        """
        åˆå§‹åŒ–èŠå¤©å›¾

        Args:
            checkpointer: LangGraphæ£€æŸ¥ç‚¹å™¨
            store: å†…å­˜å­˜å‚¨å®ä¾‹
        """
        self.checkpointer = checkpointer
        self.store = store
        self.graph = None
        self._build_graph()

    def _build_graph(self) -> None:
        """
        æ„å»ºLangGraphå¯¹è¯å›¾

        åŸºäºæœ€ä½³å®è·µæ„å»ºç®€æ´çš„å›¾ç»“æ„ï¼š
        1. ä½¿ç”¨æ ‡å‡†èŠ‚ç‚¹å‘½åï¼ˆagent, toolsï¼‰
        2. æ¸…æ™°çš„æ¡ä»¶è·¯ç”±
        3. å·¥å…·èŠ‚ç‚¹è‡ªåŠ¨å¤„ç†å¹¶è¡Œè°ƒç”¨

        å›¾æµç¨‹: START -> agent -> [æ¡ä»¶è·¯ç”±] -> {tools, END}
        tools -> agent -> [æ¡ä»¶è·¯ç”±] -> {tools, END}
        """
        try:
            # åˆ›å»ºå·¥å…·èŠ‚ç‚¹ - æ”¯æŒå¹¶è¡Œå·¥å…·è°ƒç”¨ï¼ŒåŒ…å«æ‰€æœ‰8ä¸ªå·¥å…·
            tool_node = ToolNode([
                sesame_opener,  # åŸºç¡€å·¥å…·
                query_tasks, get_task_detail,  # ä»»åŠ¡æŸ¥è¯¢å·¥å…·
                create_task, update_task, delete_task,  # ä»»åŠ¡CRUDå·¥å…·
                search_tasks,  # ä»»åŠ¡æœç´¢å·¥å…·
                batch_create_subtasks  # æ‰¹é‡æ“ä½œå·¥å…·
            ])

            # åˆ›å»ºçŠ¶æ€å›¾æ„å»ºå™¨
            builder = StateGraph(ChatState)

            # æ·»åŠ èŠ‚ç‚¹
            builder.add_node("agent", self._agent_node)
            builder.add_node("tools", tool_node)

            # æ·»åŠ è¾¹
            builder.add_edge(START, "agent")

            # æ·»åŠ æ¡ä»¶è¾¹ï¼šä½¿ç”¨æ ‡å‡†è·¯ç”±æ¨¡å¼
            builder.add_conditional_edges(
                "agent",
                self._route_to_tools,
                {
                    "tools": "tools",
                    "end": END
                }
            )

            # å·¥å…·æ‰§è¡Œå®Œæˆåè¿”å›agent
            builder.add_edge("tools", "agent")

            # ç¼–è¯‘å›¾ - åŒ…å«æ£€æŸ¥ç‚¹å’Œå­˜å‚¨
            self.graph = builder.compile(
                checkpointer=self.checkpointer,
                store=self.store
            )

            logger.info("âœ… èŠå¤©å›¾æ„å»ºæˆåŠŸï¼ˆåŸºäºæœ€ä½³å®è·µï¼‰")

        except Exception as e:
            logger.error(f"âŒ èŠå¤©å›¾æ„å»ºå¤±è´¥: {e}")
            raise

    def _agent_node(self, state: ChatState, config: RunnableConfig) -> Dict[str, Any]:
        """
        AgentèŠ‚ç‚¹ï¼šå¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼Œç”ŸæˆAIå›å¤

        åŸºäºLangGraphæœ€ä½³å®è·µï¼š
        1. ç›´æ¥ä½¿ç”¨çŠ¶æ€ä¸­çš„æ¶ˆæ¯
        2. æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        3. ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†ä¼˜åŒ–å†å²æ¶ˆæ¯
        4. è®©æ¨¡å‹å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
        5. è¿”å›æ ‡å‡†æ¶ˆæ¯æ ¼å¼

        Args:
            state: å½“å‰èŠå¤©çŠ¶æ€ï¼ˆåŒ…å«messageså­—æ®µï¼‰
            config: è¿è¡Œé…ç½®ï¼ŒåŒ…å«user_idå’Œthread_id

        Returns:
            Dict[str, Any]: æ›´æ–°åçš„çŠ¶æ€ï¼ˆä»…åŒ…å«messagesï¼‰
        """
        try:
            # ä»é…ç½®ä¸­è·å–ç”¨æˆ·ä¿¡æ¯
            user_id = config.get("configurable", {}).get("user_id")
            session_id = config.get("configurable", {}).get("thread_id")

            if not user_id or not session_id:
                raise ValueError("ç¼ºå°‘user_idæˆ–thread_idé…ç½®")

            # è·å–æ¨¡å‹ï¼ˆå·²ç»‘å®šå·¥å…·ï¼‰
            model = self._get_model()
            model_name = model.model_name if hasattr(model, 'model_name') else "gpt-3.5-turbo"

            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ - ä½¿ç”¨æ ‡å‡†çš„LangChainæ¶ˆæ¯æ ¼å¼
            messages = state["messages"]

            # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¼˜åŒ–æ¶ˆæ¯å†å²
            if len(messages) > 1:  # åªæœ‰å¤šæ¡æ¶ˆæ¯æ—¶æ‰éœ€è¦ä¼˜åŒ–
                original_count = len(messages)
                messages = manage_conversation_context(messages, model_name)
                optimized_count = len(messages)

                if original_count != optimized_count:
                    logger.info(f"ğŸ“ ä¸Šä¸‹æ–‡ä¼˜åŒ–: {original_count} -> {optimized_count} æ¡æ¶ˆæ¯")

            # æ·»åŠ ç³»ç»Ÿæç¤ºè¯åˆ°æ¶ˆæ¯å¼€å¤´
            system_prompt = format_system_prompt(user_id, session_id)
            from langchain_core.messages import SystemMessage
            messages_with_system = [SystemMessage(content=system_prompt)] + messages

            # è°ƒç”¨æ¨¡å‹ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·
            response = model.invoke(messages_with_system)

            logger.info(f"âœ… AgentèŠ‚ç‚¹å¤„ç†å®Œæˆ: user_id={user_id}, session_id={session_id}")
            logger.debug(f"ğŸ”§ user_idä¼ é€’çŠ¶æ€éªŒè¯: {user_id} -> ChatState")

            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            if hasattr(response, 'tool_calls') and response.tool_calls:
                logger.info(f"ğŸ”§ æ¨¡å‹ç”Ÿæˆå·¥å…·è°ƒç”¨: {[call['name'] for call in response.tool_calls]}")

            # è¿”å›æ›´æ–°åçš„æ¶ˆæ¯åˆ—è¡¨
            return {"messages": [response]}

        except Exception as e:
            logger.error(f"âŒ AgentèŠ‚ç‚¹å¤„ç†å¤±è´¥: {e}")

            # ç”Ÿæˆé”™è¯¯å›å¤
            from langchain_core.messages import AIMessage
            error_message = AIMessage(content="æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚")
            return {"messages": [error_message]}

    
    def _route_to_tools(self, state: ChatState) -> Literal["tools", "end"]:
        """
        è·¯ç”±å†³ç­–ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·

        ä½¿ç”¨LangGraphæœ€ä½³å®è·µï¼šæ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦æœ‰tool_calls

        Args:
            state: å½“å‰èŠå¤©çŠ¶æ€

        Returns:
            Literal["tools", "end"]: è·¯ç”±å†³ç­–
        """
        try:
            logger.info("ğŸš¦ å¼€å§‹è·¯ç”±å†³ç­–...")

            # è·å–æœ€åä¸€æ¡æ¶ˆæ¯
            last_message = state["messages"][-1] if state["messages"] else None

            if not last_message:
                logger.info("ğŸš¦ æ— æ¶ˆæ¯ï¼Œè·¯ç”±åˆ°ç»“æŸ")
                return "end"

            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ - ä½¿ç”¨æ ‡å‡†LangGraphæ¨¡å¼
            if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                logger.info(f"ğŸš¦ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {[call['name'] for call in last_message.tool_calls]}")
                return "tools"

            logger.info("ğŸš¦ æ— å·¥å…·è°ƒç”¨éœ€æ±‚ï¼Œè·¯ç”±åˆ°ç»“æŸ")
            return "end"

        except Exception as e:
            logger.error(f"è·¯ç”±å†³ç­–å¤±è´¥: {e}")
            return "end"

    
    def _get_model(self) -> ChatOpenAI:
        """
        è·å–OpenAIæ¨¡å‹å®ä¾‹å¹¶ç»‘å®šå·¥å…·

        Returns:
            ChatOpenAI: æ¨¡å‹å®ä¾‹
        """
        try:
            # èŠå¤©æ¨¡å—ä¼˜å…ˆä½¿ç”¨OpenAIé…ç½®ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰
            api_key = os.getenv("OPENAI_API_KEY") or os.getenv("LLM_API_KEY")
            base_url = os.getenv("OPENAI_BASE_URL") or os.getenv("LLM_BASE_URL", "https://api.openai.com/v1")
            model_name = os.getenv("OPENAI_MODEL") or os.getenv("LLM_MODEL", "gpt-3.5-turbo")
            temperature = float(os.getenv("OPENAI_TEMPERATURE", os.getenv("LLM_TEMPERATURE", "0.7")))

            if not api_key:
                raise ValueError("APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·è®¾ç½®LLM_API_KEYæˆ–OPENAI_API_KEYç¯å¢ƒå˜é‡")

            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            model = ChatOpenAI(
                model=model_name,
                api_key=api_key,
                base_url=base_url,
                temperature=temperature,
                max_tokens=1000
            )

            # ç»‘å®šå·¥å…· - åªå¯¹æ”¯æŒå·¥å…·è°ƒç”¨çš„æ¨¡å‹ç»‘å®š
            if "gpt" in model_name.lower() or "openai" in model_name.lower():
                try:
                    # ç»‘å®šæ‰€æœ‰8ä¸ªå·¥å…·
                    all_tools = [
                        sesame_opener,  # åŸºç¡€å·¥å…·
                        query_tasks, get_task_detail,  # ä»»åŠ¡æŸ¥è¯¢å·¥å…·
                        create_task, update_task, delete_task,  # ä»»åŠ¡CRUDå·¥å…·
                        search_tasks,  # ä»»åŠ¡æœç´¢å·¥å…·
                        batch_create_subtasks  # æ‰¹é‡æ“ä½œå·¥å…·
                    ]
                    model = model.bind_tools(all_tools)
                    logger.info(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸï¼ˆå¸¦8ä¸ªå·¥å…·ï¼‰: {model_name} @ {base_url}")
                except Exception as tool_error:
                    logger.warning(f"âš ï¸ å·¥å…·ç»‘å®šå¤±è´¥ï¼Œä½¿ç”¨ä¸å¸¦å·¥å…·çš„æ¨¡å‹: {tool_error}")
                    logger.info(f"ğŸ“ æ¨¡å‹åˆ›å»ºæˆåŠŸï¼ˆä¸å¸¦å·¥å…·ï¼‰: {model_name} @ {base_url}")
            else:
                logger.info(f"ğŸ“ æ¨¡å‹ {model_name} å¯èƒ½ä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹")

            return model

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            raise

    def invoke(self, state: ChatState, config: RunnableConfig) -> ChatState:
        """
        è°ƒç”¨èŠå¤©å›¾

        Args:
            state: èŠå¤©çŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            ChatState: å¤„ç†åçš„çŠ¶æ€
        """
        try:
            if not self.graph:
                raise RuntimeError("èŠå¤©å›¾æœªåˆå§‹åŒ–")

            result = self.graph.invoke(state, config)
            return result

        except Exception as e:
            logger.error(f"èŠå¤©å›¾è°ƒç”¨å¤±è´¥: {e}")
            raise

    def stream(self, state: ChatState, config: RunnableConfig):
        """
        æµå¼è°ƒç”¨èŠå¤©å›¾

        Args:
            state: èŠå¤©çŠ¶æ€
            config: è¿è¡Œé…ç½®

        Yields:
            ChatState: æµå¼çŠ¶æ€æ›´æ–°
        """
        try:
            if not self.graph:
                raise RuntimeError("èŠå¤©å›¾æœªåˆå§‹åŒ–")

            for chunk in self.graph.stream(state, config):
                yield chunk

        except Exception as e:
            logger.error(f"èŠå¤©å›¾æµå¼è°ƒç”¨å¤±è´¥: {e}")
            raise


def create_chat_graph(checkpointer: SqliteSaver, store: InMemoryStore) -> ChatGraph:
    """
    åˆ›å»ºèŠå¤©å›¾å®ä¾‹

    Args:
        checkpointer: æ£€æŸ¥ç‚¹å™¨
        store: å†…å­˜å­˜å‚¨

    Returns:
        ChatGraph: èŠå¤©å›¾å®ä¾‹
    """
    try:
        graph = ChatGraph(checkpointer, store)
        logger.info("èŠå¤©å›¾å®ä¾‹åˆ›å»ºæˆåŠŸ")
        return graph

    except Exception as e:
        logger.error(f"èŠå¤©å›¾å®ä¾‹åˆ›å»ºå¤±è´¥: {e}")
        raise