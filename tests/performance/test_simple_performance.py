"""
ç®€åŒ–çš„æœåŠ¡å±‚æ€§èƒ½æµ‹è¯•

è¯¥æµ‹è¯•æ–‡ä»¶åŒ…å«å¯¹æœåŠ¡å±‚æ ¸å¿ƒç»„ä»¶çš„æ€§èƒ½æµ‹è¯•ï¼Œç”¨äºè¯†åˆ«æ€§èƒ½ç“¶é¢ˆã€‚
"""

import sys
import os
import time
import asyncio
import threading
from typing import Dict, Any, List
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from src.services.base import BaseService
from src.services.logging_config import get_logger, operation_logger, performance_logger
from src.services.exceptions import BusinessException, ValidationException

# è®¾ç½®æµ‹è¯•ç¯å¢ƒå˜é‡
os.environ['SERVICE_LOG_LEVEL'] = 'ERROR'  # å‡å°‘æ—¥å¿—è¾“å‡º
os.environ['SERVICE_LOG_CONSOLE'] = 'false'
os.environ['SERVICE_LOG_FILE'] = 'false'


class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""

    def __init__(self):
        self.results = {}

    def measure_time(self, func, *args, **kwargs):
        """æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´"""
        start_time = time.perf_counter()
        result = func(*args, **kwargs)
        end_time = time.perf_counter()
        execution_time = (end_time - start_time) * 1000  # ms
        return result, execution_time

    def measure_memory(self):
        """ç®€å•çš„å†…å­˜æµ‹é‡ï¼ˆä½¿ç”¨tracemallocï¼‰"""
        import tracemalloc
        import gc

        tracemalloc.start()
        gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶

        # æ‰§è¡Œä¸€äº›æ“ä½œæ¥åˆ†é…å†…å­˜
        dummy_data = []
        for i in range(1000):
            dummy_data.append({
                "id": i,
                "data": "x" * 100,
                "timestamp": datetime.now().isoformat()
            })

        snapshot = tracemalloc.take_snapshot()
        tracemalloc.stop()

        return len(str(dummy_data)), len(snapshot.statistics("lineno"))

    def record_result(self, test_name: str, metric_name: str, value: float, unit: str = "ms"):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        if test_name not in self.results:
            self.results[test_name] = {}
        self.results[test_name][metric_name] = f"{value:.2f}{unit}"

    def print_summary(self):
        """æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦"""
        print("\n" + "="*60)
        print("æ€§èƒ½æµ‹è¯•ç»“æœæ‘˜è¦")
        print("="*60)

        for test_name, metrics in self.results.items():
            print(f"\nğŸ” {test_name}:")
            for metric_name, value in metrics.items():
                print(f"   {metric_name}: {value}")


class TestServiceCreationPerformance:
    """æœåŠ¡åˆ›å»ºæ€§èƒ½æµ‹è¯•"""

    def __init__(self):
        self.analyzer = PerformanceAnalyzer()

    def test_service_instantiation(self):
        """æµ‹è¯•æœåŠ¡å®ä¾‹åŒ–æ€§èƒ½"""
        print("\n=== æœåŠ¡å®ä¾‹åŒ–æ€§èƒ½æµ‹è¯• ===")

        # æµ‹è¯•BaseServiceå®ä¾‹åŒ–
        times = []
        for i in range(50):
            _, exec_time = self.analyzer.measure_time(BaseService)
            times.append(exec_time)

        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)

        self.analyzer.record_result("BaseServiceå®ä¾‹åŒ–", "å¹³å‡æ—¶é—´", avg_time)
        self.analyzer.record_result("BaseServiceå®ä¾‹åŒ–", "æœ€å°æ—¶é—´", min_time)
        self.analyzer.record_result("BaseServiceå®ä¾‹åŒ–", "æœ€å¤§æ—¶é—´", max_time)

        print(f"BaseServiceå®ä¾‹åŒ–æ€§èƒ½:")
        print(f"  å¹³å‡æ—¶é—´: {avg_time:.2f}ms")
        print(f"  æœ€å¿«æ—¶é—´: {min_time:.2f}ms")
        print(f"  æœ€æ…¢æ—¶é—´: {max_time:.2f}ms")

        # æ€§èƒ½è¯„ä¼°
        if avg_time > 10:
            print("âš ï¸ æœåŠ¡å®ä¾‹åŒ–è¾ƒæ…¢ï¼Œéœ€è¦ä¼˜åŒ–")
        else:
            print("âœ… æœåŠ¡å®ä¾‹åŒ–æ€§èƒ½è‰¯å¥½")

        return avg_time

    def test_multiple_services_creation(self):
        """æµ‹è¯•å¤šä¸ªæœåŠ¡åˆ›å»ºæ€§èƒ½"""
        print("\n=== å¤šæœåŠ¡åˆ›å»ºæ€§èƒ½æµ‹è¯• ===")

        times = []
        for i in range(30):
            _, exec_time = self.analyzer.measure_time(self._create_multiple_services)
            times.append(exec_time)

        avg_time = sum(times) / len(times)
        self.analyzer.record_result("å¤šæœåŠ¡åˆ›å»º", "å¹³å‡æ—¶é—´", avg_time)

        print(f"åˆ›å»ºå¤šä¸ªæœåŠ¡å¹³å‡æ—¶é—´: {avg_time:.2f}ms")

        if avg_time > 50:
            print("âš ï¸ å¤šæœåŠ¡åˆ›å»ºæ€§èƒ½è¾ƒæ…¢")
        else:
            print("âœ… å¤šæœåŠ¡åˆ›å»ºæ€§èƒ½è‰¯å¥½")

        return avg_time

    def _create_multiple_services(self):
        """åˆ›å»ºå¤šä¸ªæœåŠ¡å®ä¾‹"""
        services = []
        for i in range(5):
            service = BaseService()
            services.append(service)
        return services


class TestLoggingSystemPerformance:
    """æ—¥å¿—ç³»ç»Ÿæ€§èƒ½æµ‹è¯•"""

    def __init__(self):
        self.analyzer = PerformanceAnalyzer()
        self.service = BaseService()

    def test_basic_logging(self):
        """æµ‹è¯•åŸºæœ¬æ—¥å¿—æ€§èƒ½"""
        print("\n=== åŸºæœ¬æ—¥å¿—æ€§èƒ½æµ‹è¯• ===")

        # æµ‹è¯•INFOæ—¥å¿—
        info_times = []
        for i in range(100):
            _, exec_time = self.analyzer.measure_time(
                self.service._log_info, f"æµ‹è¯•æ—¥å¿—æ¶ˆæ¯ {i}", test_data=i
            )
            info_times.append(exec_time)

        avg_info_time = sum(info_times) / len(info_times)
        self.analyzer.record_result("INFOæ—¥å¿—", "å¹³å‡æ—¶é—´", avg_info_time, "Î¼s")

        # æµ‹è¯•ERRORæ—¥å¿—
        error_times = []
        test_exception = Exception("æµ‹è¯•å¼‚å¸¸")
        for i in range(50):
            _, exec_time = self.analyzer.measure_time(
                self.service._log_error, f"æµ‹è¯•é”™è¯¯æ—¥å¿— {i}", error=test_exception
            )
            error_times.append(exec_time)

        avg_error_time = sum(error_times) / len(error_times)
        self.analyzer.record_result("ERRORæ—¥å¿—", "å¹³å‡æ—¶é—´", avg_error_time)

        print(f"INFOæ—¥å¿—å¹³å‡æ—¶é—´: {avg_info_time*1000:.2f}Î¼s")
        print(f"ERRORæ—¥å¿—å¹³å‡æ—¶é—´: {avg_error_time:.2f}ms")

        # æ€§èƒ½è¯„ä¼°
        if avg_info_time > 5:  # 5msé˜ˆå€¼
            print("âš ï¸ INFOæ—¥å¿—æ€§èƒ½è¾ƒæ…¢")
        else:
            print("âœ… INFOæ—¥å¿—æ€§èƒ½è‰¯å¥½")

        if avg_error_time > 10:  # 10msé˜ˆå€¼
            print("âš ï¸ ERRORæ—¥å¿—æ€§èƒ½è¾ƒæ…¢")
        else:
            print("âœ… ERRORæ—¥å¿—æ€§èƒ½è‰¯å¥½")

        return {"info": avg_info_time, "error": avg_error_time}

    def test_operation_logging(self):
        """æµ‹è¯•æ“ä½œæ—¥å¿—æ€§èƒ½"""
        print("\n=== æ“ä½œæ—¥å¿—æ€§èƒ½æµ‹è¯• ===")

        operation_times = []
        for i in range(50):
            _, exec_time = self.analyzer.measure_time(
                self._log_operation_cycle, f"æµ‹è¯•æ“ä½œ {i}", f"user_{i}"
            )
            operation_times.append(exec_time)

        avg_time = sum(operation_times) / len(operation_times)
        self.analyzer.record_result("æ“ä½œæ—¥å¿—", "å¹³å‡æ—¶é—´", avg_time)

        print(f"æ“ä½œæ—¥å¿—å¹³å‡æ—¶é—´: {avg_time:.2f}ms")

        if avg_time > 20:
            print("âš ï¸ æ“ä½œæ—¥å¿—æ€§èƒ½è¾ƒæ…¢")
        else:
            print("âœ… æ“ä½œæ—¥å¿—æ€§èƒ½è‰¯å¥½")

        return avg_time

    def _log_operation_cycle(self, operation_name: str, user_id: str):
        """å®Œæ•´çš„æ“ä½œæ—¥å¿—å‘¨æœŸ"""
        self.service._log_operation_start(operation_name, user_id=user_id)
        self.service._log_operation_success(operation_name, duration_ms=100, user_id=user_id)

    def test_logging_decorators(self):
        """æµ‹è¯•æ—¥å¿—è£…é¥°å™¨æ€§èƒ½"""
        print("\n=== æ—¥å¿—è£…é¥°å™¨æ€§èƒ½æµ‹è¯• ===")

        class TestService(BaseService):
            @operation_logger("è£…é¥°å™¨æµ‹è¯•", log_args=True, log_result=True)
            def test_method(self, data: Dict[str, Any]) -> Dict[str, Any]:
                return {"processed": True, "data": data}

            @performance_logger("æ€§èƒ½è£…é¥°å™¨æµ‹è¯•")
            def performance_test(self, size: int) -> int:
                return sum(range(size))

        test_service = TestService()

        # æµ‹è¯•æ“ä½œæ—¥å¿—è£…é¥°å™¨
        decorator_times = []
        test_data = {"key": "value", "number": 42}

        for i in range(30):
            _, exec_time = self.analyzer.measure_time(
                test_service.test_method, test_data
            )
            decorator_times.append(exec_time)

        avg_decorator_time = sum(decorator_times) / len(decorator_times)
        self.analyzer.record_result("æ“ä½œæ—¥å¿—è£…é¥°å™¨", "å¹³å‡æ—¶é—´", avg_decorator_time)

        print(f"æ“ä½œæ—¥å¿—è£…é¥°å™¨å¹³å‡æ—¶é—´: {avg_decorator_time:.2f}ms")

        if avg_decorator_time > 15:
            print("âš ï¸ æ“ä½œæ—¥å¿—è£…é¥°å™¨æ€§èƒ½è¾ƒæ…¢")
        else:
            print("âœ… æ“ä½œæ—¥å¿—è£…é¥°å™¨æ€§èƒ½è‰¯å¥½")

        # æµ‹è¯•æ€§èƒ½æ—¥å¿—è£…é¥°å™¨
        perf_times = []
        for i in range(30):
            _, exec_time = self.analyzer.measure_time(
                test_service.performance_test, 100
            )
            perf_times.append(exec_time)

        avg_perf_time = sum(perf_times) / len(perf_times)
        self.analyzer.record_result("æ€§èƒ½æ—¥å¿—è£…é¥°å™¨", "å¹³å‡æ—¶é—´", avg_perf_time)

        print(f"æ€§èƒ½æ—¥å¿—è£…é¥°å™¨å¹³å‡æ—¶é—´: {avg_perf_time:.2f}ms")

        if avg_perf_time > 10:
            print("âš ï¸ æ€§èƒ½æ—¥å¿—è£…é¥°å™¨è¾ƒæ…¢")
        else:
            print("âœ… æ€§èƒ½æ—¥å¿—è£…é¥°å™¨è‰¯å¥½")

        return {
            "operation_decorator": avg_decorator_time,
            "performance_decorator": avg_perf_time
        }


class TestExceptionHandlingPerformance:
    """å¼‚å¸¸å¤„ç†æ€§èƒ½æµ‹è¯•"""

    def __init__(self):
        self.analyzer = PerformanceAnalyzer()
        self.service = BaseService()

    def test_exception_creation(self):
        """æµ‹è¯•å¼‚å¸¸åˆ›å»ºæ€§èƒ½"""
        print("\n=== å¼‚å¸¸åˆ›å»ºæ€§èƒ½æµ‹è¯• ===")

        # æµ‹è¯•BusinessExceptionåˆ›å»º
        exception_times = []
        for i in range(100):
            _, exec_time = self.analyzer.measure_time(
                BusinessException, "TEST_ERROR", "æµ‹è¯•æ¶ˆæ¯", user_message="ç”¨æˆ·å‹å¥½æ¶ˆæ¯"
            )
            exception_times.append(exec_time)

        avg_time = sum(exception_times) / len(exception_times)
        self.analyzer.record_result("å¼‚å¸¸åˆ›å»º", "å¹³å‡æ—¶é—´", avg_time, "Î¼s")

        print(f"å¼‚å¸¸åˆ›å»ºå¹³å‡æ—¶é—´: {avg_time*1000:.2f}Î¼s")

        if avg_time > 50:  # 50å¾®ç§’é˜ˆå€¼
            print("âš ï¸ å¼‚å¸¸åˆ›å»ºæ€§èƒ½è¾ƒæ…¢")
        else:
            print("âœ… å¼‚å¸¸åˆ›å»ºæ€§èƒ½è‰¯å¥½")

        return avg_time

    def test_exception_handling_overhead(self):
        """æµ‹è¯•å¼‚å¸¸å¤„ç†å¼€é”€"""
        print("\n=== å¼‚å¸¸å¤„ç†å¼€é”€æµ‹è¯• ===")

        # æµ‹è¯•æ­£å¸¸æ‰§è¡Œ
        normal_times = []
        for i in range(50):
            _, exec_time = self.analyzer.measure_time(
                self._validate_data_normal, {"name": "test", "value": i}
            )
            normal_times.append(exec_time)

        avg_normal_time = sum(normal_times) / len(normal_times)

        # æµ‹è¯•å¼‚å¸¸å¤„ç†
        exception_times = []
        for i in range(50):
            _, exec_time = self.analyzer.measure_time(
                self._validate_data_with_exception, {"name": "test"}  # ç¼ºå°‘valueå­—æ®µ
            )
            exception_times.append(exec_time)

        avg_exception_time = sum(exception_times) / len(exception_times)

        overhead = ((avg_exception_time / avg_normal_time) - 1) * 100

        self.analyzer.record_result("æ­£å¸¸æ‰§è¡Œ", "å¹³å‡æ—¶é—´", avg_normal_time)
        self.analyzer.record_result("å¼‚å¸¸å¤„ç†", "å¹³å‡æ—¶é—´", avg_exception_time)
        self.analyzer.record_result("å¼‚å¸¸å¤„ç†", "å¼€é”€ç™¾åˆ†æ¯”", overhead)

        print(f"æ­£å¸¸æ‰§è¡Œå¹³å‡æ—¶é—´: {avg_normal_time:.2f}ms")
        print(f"å¼‚å¸¸å¤„ç†å¹³å‡æ—¶é—´: {avg_exception_time:.2f}ms")
        print(f"å¼‚å¸¸å¤„ç†å¼€é”€: {overhead:.1f}%")

        if overhead > 200:  # 200%é˜ˆå€¼
            print("âš ï¸ å¼‚å¸¸å¤„ç†å¼€é”€è¿‡å¤§")
        else:
            print("âœ… å¼‚å¸¸å¤„ç†å¼€é”€åœ¨å¯æ¥å—èŒƒå›´å†…")

        return {
            "normal_time": avg_normal_time,
            "exception_time": avg_exception_time,
            "overhead": overhead
        }

    def _validate_data_normal(self, data: Dict[str, Any]) -> bool:
        """æ­£å¸¸çš„æ•°æ®éªŒè¯"""
        return "name" in data and "value" in data

    def _validate_data_with_exception(self, data: Dict[str, Any]) -> bool:
        """ä¼šæŠ›å‡ºå¼‚å¸¸çš„æ•°æ®éªŒè¯"""
        try:
            if "name" not in data or "value" not in data:
                raise ValidationException("field", "value", "ç¼ºå°‘å¿…å¡«å­—æ®µ")
            return True
        except Exception:
            return False


class TestConcurrencyPerformance:
    """å¹¶å‘æ€§èƒ½æµ‹è¯•"""

    def __init__(self):
        self.analyzer = PerformanceAnalyzer()

    def test_concurrent_service_creation(self):
        """æµ‹è¯•å¹¶å‘æœåŠ¡åˆ›å»º"""
        print("\n=== å¹¶å‘æœåŠ¡åˆ›å»ºæ€§èƒ½æµ‹è¯• ===")

        def create_service_thread(results, thread_id):
            """åˆ›å»ºæœåŠ¡çš„çº¿ç¨‹å‡½æ•°"""
            thread_times = []
            for i in range(10):
                _, exec_time = self.analyzer.measure_time(BaseService)
                thread_times.append(exec_time)

            results.append({
                "thread_id": thread_id,
                "times": thread_times,
                "avg_time": sum(thread_times) / len(thread_times)
            })

        # åˆ›å»ºå¤šä¸ªçº¿ç¨‹
        threads = []
        results = []

        for i in range(5):
            thread = threading.Thread(
                target=create_service_thread,
                args=(results, i)
            )
            threads.append(thread)

        # å¯åŠ¨æ‰€æœ‰çº¿ç¨‹
        start_time = time.perf_counter()
        for thread in threads:
            thread.start()

        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        end_time = time.perf_counter()

        total_time = (end_time - start_time) * 1000
        total_operations = sum(len(r["times"]) for r in results)
        avg_thread_time = sum(r["avg_time"] for r in results) / len(results)

        print(f"æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}ms")
        print(f"æ€»æ“ä½œæ•°: {total_operations}")
        print(f"æ¯çº¿ç¨‹å¹³å‡æ—¶é—´: {avg_thread_time:.2f}ms")
        print(f"å¹¶å‘ååé‡: {total_operations / (total_time / 1000):.2f} æ“ä½œ/ç§’")

        self.analyzer.record_result("å¹¶å‘æœåŠ¡åˆ›å»º", "æ€»æ—¶é—´", total_time)
        self.analyzer.record_result("å¹¶å‘æœåŠ¡åˆ›å»º", "æ“ä½œæ•°", total_operations)
        self.analyzer.record_result("å¹¶å‘æœåŠ¡åˆ›å»º", "ååé‡", total_operations / (total_time / 1000), "æ“ä½œ/ç§’")

        return total_operations / (total_time / 1000)

    def test_concurrent_logging(self):
        """æµ‹è¯•å¹¶å‘æ—¥å¿—è®°å½•"""
        print("\n=== å¹¶å‘æ—¥å¿—è®°å½•æ€§èƒ½æµ‹è¯• ===")

        def logging_thread(service, results, thread_id):
            """æ—¥å¿—è®°å½•çº¿ç¨‹å‡½æ•°"""
            thread_times = []
            for i in range(50):
                _, exec_time = self.analyzer.measure_time(
                    service._log_info, f"çº¿ç¨‹{thread_id}æ—¥å¿—{i}",
                    thread_id=thread_id,
                    iteration=i
                )
                thread_times.append(exec_time)

            results.append({
                "thread_id": thread_id,
                "avg_time": sum(thread_times) / len(thread_times)
            })

        service = BaseService()
        threads = []
        results = []

        # åˆ›å»ºå¤šä¸ªæ—¥å¿—çº¿ç¨‹
        for i in range(3):
            thread = threading.Thread(
                target=logging_thread,
                args=(service, results, i)
            )
            threads.append(thread)

        # å¯åŠ¨æ‰€æœ‰çº¿ç¨‹
        start_time = time.perf_counter()
        for thread in threads:
            thread.start()

        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        end_time = time.perf_counter()

        total_time = (end_time - start_time) * 1000
        total_logs = sum(len(r.get("times", [])) for r in results)
        avg_thread_time = sum(r["avg_time"] for r in results) / len(results)

        print(f"æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}ms")
        print(f"æ€»æ—¥å¿—æ•°: {total_logs}")
        print(f"æ¯çº¿ç¨‹å¹³å‡æ—¶é—´: {avg_thread_time:.2f}ms")
        print(f"å¹¶å‘æ—¥å¿—ååé‡: {total_logs / (total_time / 1000):.2f} æ—¥å¿—/ç§’")

        self.analyzer.record_result("å¹¶å‘æ—¥å¿—è®°å½•", "ååé‡", total_logs / (total_time / 1000), "æ—¥å¿—/ç§’")

        return total_logs / (total_time / 1000)


class TestMemoryUsage:
    """å†…å­˜ä½¿ç”¨æµ‹è¯•"""

    def __init__(self):
        self.analyzer = PerformanceAnalyzer()

    def test_service_memory_usage(self):
        """æµ‹è¯•æœåŠ¡å†…å­˜ä½¿ç”¨"""
        print("\n=== æœåŠ¡å†…å­˜ä½¿ç”¨æµ‹è¯• ===")

        # æµ‹è¯•å•ä¸ªæœåŠ¡å®ä¾‹çš„å†…å­˜ä½¿ç”¨
        services = []
        memory_info = []

        for i in range(100):
            service = BaseService()
            services.append(service)

            # æ¯20ä¸ªå®ä¾‹æ£€æŸ¥ä¸€æ¬¡å†…å­˜
            if (i + 1) % 20 == 0:
                items, lines = self.analyzer.measure_memory()
                memory_info.append({
                    "count": len(services),
                    "items": items,
                    "lines": lines
                })

        print(f"åˆ›å»º100ä¸ªæœåŠ¡å®ä¾‹çš„å†…å­˜ä½¿ç”¨:")
        for info in memory_info:
            print(f"  {info['count']}ä¸ªå®ä¾‹: {info['items']}ä¸ªé¡¹ç›®, {info['lines']}è¡Œ")

        # æ¸…ç†
        del services
        import gc
        gc.collect()

        # æ£€æŸ¥å†…å­˜æ˜¯å¦é‡Šæ”¾
        final_items, final_lines = self.analyzer.measure_memory()
        print(f"æ¸…ç†åå†…å­˜ä½¿ç”¨: {final_items}ä¸ªé¡¹ç›®, {final_lines}è¡Œ")

        if final_lines > 1000:
            print("âš ï¸ å¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼")
        else:
            print("âœ… å†…å­˜ä½¿ç”¨æ­£å¸¸")

        return memory_info

    def test_logging_memory_impact(self):
        """æµ‹è¯•æ—¥å¿—å¯¹å†…å­˜çš„å½±å“"""
        print("\n=== æ—¥å¿—å†…å­˜å½±å“æµ‹è¯• ===")

        service = BaseService()

        # æµ‹è¯•å¤§é‡æ—¥å¿—è®°å½•çš„å†…å­˜å½±å“
        print("è®°å½•å¤§é‡æ—¥å¿—...")
        for i in range(1000):
            service._log_info(f"å†…å­˜æµ‹è¯•æ—¥å¿— {i}",
                            iteration=i,
                            large_data="x" * 50)  # åŒ…å«ä¸€äº›æ•°æ®

        items, lines = self.analyzer.measure_memory()
        print(f"1000æ¡æ—¥å¿—åå†…å­˜ä½¿ç”¨: {items}ä¸ªé¡¹ç›®, {lines}è¡Œ")

        # æµ‹è¯•å¼‚å¸¸æ—¥å¿—çš„å†…å­˜å½±å“
        print("è®°å½•å¤§é‡å¼‚å¸¸æ—¥å¿—...")
        test_exception = Exception("å†…å­˜æµ‹è¯•å¼‚å¸¸")
        for i in range(100):
            service._log_error(f"å†…å­˜æµ‹è¯•é”™è¯¯ {i}",
                            error=test_exception,
                            iteration=i)

        items, lines = self.analyzer.measure_memory()
        print(f"100æ¡å¼‚å¸¸æ—¥å¿—åå†…å­˜ä½¿ç”¨: {items}ä¸ªé¡¹ç›®, {lines}è¡Œ")

        self.analyzer.record_result("æ—¥å¿—å†…å­˜å½±å“", "æœ€å¤§é¡¹ç›®æ•°", items)
        self.analyzer.record_result("æ—¥å¿—å†…å­˜å½±å“", "æœ€å¤§è¡Œæ•°", lines)

        if lines > 5000:
            print("âš ï¸ æ—¥å¿—ç³»ç»Ÿå†…å­˜å½±å“è¾ƒå¤§")
        else:
            print("âœ… æ—¥å¿—ç³»ç»Ÿå†…å­˜å½±å“å¯æ¥å—")

        return {"items": items, "lines": lines}


class PerformanceTestSuite:
    """å®Œæ•´æ€§èƒ½æµ‹è¯•å¥—ä»¶"""

    def __init__(self):
        self.analyzer = PerformanceAnalyzer()
        self.all_results = {}

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æœåŠ¡å±‚æ€§èƒ½æµ‹è¯•")
        print("="*60)

        # 1. æœåŠ¡åˆ›å»ºæ€§èƒ½æµ‹è¯•
        print("\nğŸ”§ ç¬¬ä¸€éƒ¨åˆ†: æœåŠ¡åˆ›å»ºæ€§èƒ½æµ‹è¯•")
        creation_test = TestServiceCreationPerformance()
        self.all_results["service_creation"] = creation_test.test_service_instantiation()
        creation_test.test_multiple_services_creation()

        # 2. æ—¥å¿—ç³»ç»Ÿæ€§èƒ½æµ‹è¯•
        print("\nğŸ“ ç¬¬äºŒéƒ¨åˆ†: æ—¥å¿—ç³»ç»Ÿæ€§èƒ½æµ‹è¯•")
        logging_test = TestLoggingSystemPerformance()
        basic_results = logging_test.test_basic_logging()
        operation_results = logging_test.test_operation_logging()
        decorator_results = logging_test.test_logging_decorators()
        self.all_results["logging_system"] = {
            **basic_results,
            "operation": operation_results,
            **decorator_results
        }

        # 3. å¼‚å¸¸å¤„ç†æ€§èƒ½æµ‹è¯•
        print("\nâš ï¸ ç¬¬ä¸‰éƒ¨åˆ†: å¼‚å¸¸å¤„ç†æ€§èƒ½æµ‹è¯•")
        exception_test = TestExceptionHandlingPerformance()
        exception_creation_time = exception_test.test_exception_creation()
        handling_results = exception_test.test_exception_handling_overhead()
        self.all_results["exception_handling"] = {
            "exception_creation": exception_creation_time,
            "normal_time": handling_results.get("normal_time", 0),
            "exception_time": handling_results.get("exception_time", 0),
            "overhead": handling_results.get("overhead", 0)
        }

        # 4. å¹¶å‘æ€§èƒ½æµ‹è¯•
        print("\nğŸ”„ ç¬¬å››éƒ¨åˆ†: å¹¶å‘æ€§èƒ½æµ‹è¯•")
        concurrency_test = TestConcurrencyPerformance()
        concurrent_service_throughput = concurrency_test.test_concurrent_service_creation()
        concurrent_logging_throughput = concurrency_test.test_concurrent_logging()
        self.all_results["concurrency"] = {
            "service_throughput": concurrent_service_throughput,
            "logging_throughput": concurrent_logging_throughput
        }

        # 5. å†…å­˜ä½¿ç”¨æµ‹è¯•
        print("\nğŸ’¾ ç¬¬äº”éƒ¨åˆ†: å†…å­˜ä½¿ç”¨æµ‹è¯•")
        memory_test = TestMemoryUsage()
        memory_results = memory_test.test_service_memory_usage()
        logging_memory_results = memory_test.test_logging_memory_impact()
        self.all_results["memory"] = {
            "service": memory_results,
            "logging": logging_memory_results
        }

        # 6. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        # å°†æ‰€æœ‰ç»“æœè½¬æ¢ä¸ºé€‚åˆæ‰“å°çš„æ ¼å¼
        formatted_results = {}
        for key, value in self.all_results.items():
            if isinstance(value, dict):
                formatted_results[key] = value
            else:
                formatted_results[key] = {"result": f"{value:.2f}"}

        self.analyzer.results.update(formatted_results)
        self.analyzer.print_summary()
        self._generate_performance_recommendations()

        print("\nâœ… æ‰€æœ‰æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
        return self.all_results

    def _generate_performance_recommendations(self):
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        print("\n" + "="*60)
        print("æ€§èƒ½ä¼˜åŒ–å»ºè®®")
        print("="*60)

        recommendations = []

        # åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        if "service_creation" in self.all_results:
            creation_time = self.all_results["service_creation"]
            if creation_time > 10:
                recommendations.append("ğŸ”§ è€ƒè™‘ä½¿ç”¨å¯¹è±¡æ± æ¥é‡ç”¨æœåŠ¡å®ä¾‹")
                recommendations.append("ğŸ”§ å»¶è¿ŸåŠ è½½éå…³é”®æœåŠ¡")

        if "logging_system" in self.all_results:
            logging_results = self.all_results["logging_system"]
            if logging_results.get("operation", 0) > 20:
                recommendations.append("ğŸ“ ä¼˜åŒ–æ“ä½œæ—¥å¿—è£…é¥°å™¨ï¼Œå‡å°‘æ—¥å¿—å¼€é”€")
            if logging_results.get("performance_decorator", 0) > 15:
                recommendations.append("ğŸ“ åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è°¨æ…ä½¿ç”¨æ€§èƒ½æ—¥å¿—è£…é¥°å™¨")

        if "exception_handling" in self.all_results:
            handling_results = self.all_results["exception_handling"]
            if handling_results.get("overhead", 0) > 200:
                recommendations.append("âš ï¸ å‡å°‘å¼‚å¸¸åˆ›å»ºé¢‘ç‡ï¼Œä¼˜åŒ–å¼‚å¸¸å¤„ç†é€»è¾‘")
                recommendations.append("âš ï¸ ä½¿ç”¨å¿«é€Ÿå¤±è´¥æ¨¡å¼ï¼Œé¿å…ä¸å¿…è¦çš„å¼‚å¸¸å¤„ç†")

        if "concurrency" in self.all_results:
            concurrency_results = self.all_results["concurrency"]
            if concurrency_results.get("service_throughput", 0) < 100:
                recommendations.append("ğŸ”„ ä¼˜åŒ–æœåŠ¡åˆ›å»ºé€»è¾‘ï¼Œæé«˜å¹¶å‘æ€§èƒ½")
            if concurrency_results.get("logging_throughput", 0) < 1000:
                recommendations.append("ğŸ“ ä¼˜åŒ–æ—¥å¿—ç³»ç»Ÿï¼Œæé«˜å¹¶å‘æ—¥å¿—æ€§èƒ½")

        if "memory" in self.all_results:
            memory_results = self.all_results["memory"]
            if memory_results.get("logging", {}).get("lines", 0) > 5000:
                recommendations.append("ğŸ’¾ ä¼˜åŒ–æ—¥å¿—ç³»ç»Ÿï¼Œå‡å°‘å†…å­˜å ç”¨")

        if recommendations:
            print("\nä¼˜åŒ–å»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                print(f"{i}. {rec}")
        else:
            print("\nğŸ‰ å½“å‰æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œæ— éœ€ç‰¹åˆ«ä¼˜åŒ–")

        print("\nğŸ“Š é•¿æœŸç›‘æ§å»ºè®®:")
        print("1. å®šæœŸè¿è¡Œæ€§èƒ½æµ‹è¯•ï¼Œç›‘æ§æ€§èƒ½å˜åŒ–")
        print("2. åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç›‘æ§å…³é”®æŒ‡æ ‡")
        print("3. æ ¹æ®ä¸šåŠ¡å¢é•¿è°ƒæ•´æ€§èƒ½ä¼˜åŒ–ç­–ç•¥")
        print("4. å»ºç«‹æ€§èƒ½åŸºçº¿å’Œå‘Šè­¦æœºåˆ¶")


def main():
    """ä¸»å‡½æ•°"""
    suite = PerformanceTestSuite()
    results = suite.run_all_tests()
    return results


if __name__ == "__main__":
    main()