"""
æœåŠ¡å±‚æ€§èƒ½æµ‹è¯•

è¯¥æµ‹è¯•æ–‡ä»¶åŒ…å«å¯¹æ•´ä¸ªæœåŠ¡å±‚çš„æ€§èƒ½æµ‹è¯•ï¼Œç”¨äºè¯†åˆ«æ€§èƒ½ç“¶é¢ˆå’Œä¼˜åŒ–æœºä¼šã€‚
æµ‹è¯•åŒ…æ‹¬ï¼š
1. æœåŠ¡åˆå§‹åŒ–æ€§èƒ½æµ‹è¯•
2. å†…å­˜ä½¿ç”¨åˆ†æ
3. æ—¥å¿—ç³»ç»Ÿæ€§èƒ½æµ‹è¯•
4. å¼‚å¸¸å¤„ç†æ€§èƒ½æµ‹è¯•
5. å¹¶å‘è®¿é—®æ€§èƒ½æµ‹è¯•
6. é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§æµ‹è¯•
"""

import pytest
import asyncio
import time
import tracemalloc
import psutil
import os
import threading
from typing import List, Dict, Any
from datetime import datetime
from unittest.mock import Mock, AsyncMock

from src.services.auth_service import AuthService
from src.services.user_service import UserService
from src.services.task_service import TaskService
from src.services.focus_service import FocusService
from src.services.reward_service import RewardService
from src.services.statistics_service import StatisticsService
from src.services.chat_service import ChatService
from src.services.base import BaseService, ServiceFactory

pytestmark = pytest.mark.asyncio


class PerformanceTestBase:
    """æ€§èƒ½æµ‹è¯•åŸºç±»"""

    @staticmethod
    def measure_memory_usage():
        """æµ‹é‡å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        process = psutil.Process()
        return {
            "rss": process.memory_info().rss / 1024 / 1024,  # MB
            "vms": process.memory_info().vms / 1024 / 1024,  # MB
            "percent": process.memory_percent()
        }

    @staticmethod
    def measure_cpu_usage():
        """æµ‹é‡CPUä½¿ç”¨æƒ…å†µ"""
        process = psutil.Process()
        return process.cpu_percent()

    @staticmethod
    def start_memory_tracing():
        """å¼€å§‹å†…å­˜è·Ÿè¸ª"""
        tracemalloc.start()

    @staticmethod
    def stop_memory_tracing():
        """åœæ­¢å†…å­˜è·Ÿè¸ªå¹¶è¿”å›ç»“æœ"""
        snapshot = tracemalloc.take_snapshot()
        tracemalloc.stop()
        return snapshot


class TestServiceInitializationPerformance(PerformanceTestBase):
    """æœåŠ¡åˆå§‹åŒ–æ€§èƒ½æµ‹è¯•"""

    def test_service_initialization_performance(self):
        """æµ‹è¯•æœåŠ¡åˆå§‹åŒ–æ€§èƒ½"""
        print("\n=== æœåŠ¡åˆå§‹åŒ–æ€§èƒ½æµ‹è¯• ===")

        services_to_test = [
            (AuthService, "AuthService"),
            (UserService, "UserService"),
            (TaskService, "TaskService"),
            (FocusService, "FocusService"),
            (RewardService, "RewardService"),
            (StatisticsService, "StatisticsService"),
            (ChatService, "ChatService")
        ]

        results = []

        for service_class, service_name in services_to_test:
            print(f"\næµ‹è¯• {service_name} åˆå§‹åŒ–æ€§èƒ½...")

            # é¢„çƒ­
            for _ in range(3):
                service = service_class()
                del service

            # æ­£å¼æµ‹è¯•
            initialization_times = []
            memory_usage_before = []

            for i in range(10):
                # è®°å½•åˆå§‹åŒ–å‰å†…å­˜
                memory_before = self.measure_memory_usage()
                memory_usage_before.append(memory_before)

                # æµ‹é‡åˆå§‹åŒ–æ—¶é—´
                start_time = time.perf_counter()
                service = service_class()
                end_time = time.perf_counter()

                init_time = (end_time - start_time) * 1000  # ms
                initialization_times.append(init_time)

                # æ¸…ç†
                del service

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            avg_time = sum(initialization_times) / len(initialization_times)
            min_time = min(initialization_times)
            max_time = max(initialization_times)

            avg_memory_before = sum(m["rss"] for m in memory_usage_before) / len(memory_usage_before)

            results.append({
                "service": service_name,
                "avg_time_ms": avg_time,
                "min_time_ms": min_time,
                "max_time_ms": max_time,
                "avg_memory_mb": avg_memory_before,
                "iterations": len(initialization_times)
            })

            print(f"  {service_name}:")
            print(f"    å¹³å‡åˆå§‹åŒ–æ—¶é—´: {avg_time:.2f}ms")
            print(f"    æœ€å¿«åˆå§‹åŒ–æ—¶é—´: {min_time:.2f}ms")
            print(f"    æœ€æ…¢åˆå§‹åŒ–æ—¶é—´: {max_time:.2f}ms")
            print(f"    å¹³å‡å†…å­˜ä½¿ç”¨: {avg_memory_before:.2f}MB")

        # æ€§èƒ½è¯„ä¼°
        print(f"\n=== åˆå§‹åŒ–æ€§èƒ½è¯„ä¼° ===")
        for result in results:
            if result["avg_time_ms"] > 100:  # 100msé˜ˆå€¼
                print(f"âš ï¸  {result['service']} åˆå§‹åŒ–è¾ƒæ…¢: {result['avg_time_ms']:.2f}ms")
            else:
                print(f"âœ… {result['service']} åˆå§‹åŒ–æ€§èƒ½è‰¯å¥½: {result['avg_time_ms']:.2f}ms")

        return results

    def test_service_factory_performance(self):
        """æµ‹è¯•ServiceFactoryæ€§èƒ½"""
        print("\n=== ServiceFactoryæ€§èƒ½æµ‹è¯• ===")

        factory_times = []
        memory_usage = []

        for i in range(20):
            memory_before = self.measure_memory_usage()
            memory_usage.append(memory_before)

            start_time = time.perf_counter()
            service = ServiceFactory.create_service(UserService)
            end_time = time.perf_counter()

            factory_time = (end_time - start_time) * 1000
            factory_times.append(factory_time)

            del service

        avg_time = sum(factory_times) / len(factory_times)
        avg_memory = sum(m["rss"] for m in memory_usage) / len(memory_usage)

        print(f"ServiceFactoryå¹³å‡åˆ›å»ºæ—¶é—´: {avg_time:.2f}ms")
        print(f"å¹³å‡å†…å­˜ä½¿ç”¨: {avg_memory:.2f}MB")

        if avg_time > 50:
            print("âš ï¸ ServiceFactoryåˆ›å»ºæ€§èƒ½éœ€è¦ä¼˜åŒ–")
        else:
            print("âœ… ServiceFactoryæ€§èƒ½è‰¯å¥½")


class TestLoggingSystemPerformance(PerformanceTestBase):
    """æ—¥å¿—ç³»ç»Ÿæ€§èƒ½æµ‹è¯•"""

    def test_logging_performance(self):
        """æµ‹è¯•æ—¥å¿—ç³»ç»Ÿæ€§èƒ½"""
        print("\n=== æ—¥å¿—ç³»ç»Ÿæ€§èƒ½æµ‹è¯• ===")

        # è®¾ç½®æ—¥å¿—çº§åˆ«ä¸ºDEBUGä»¥æµ‹è¯•æ‰€æœ‰æ—¥å¿—
        os.environ['SERVICE_LOG_LEVEL'] = 'DEBUG'
        os.environ['SERVICE_LOG_CONSOLE'] = 'false'  # å…³é—­æ§åˆ¶å°è¾“å‡ºé¿å…å¹²æ‰°
        os.environ['SERVICE_LOG_FILE'] = 'false'      # å…³é—­æ–‡ä»¶è¾“å‡º

        service = UserService()

        # æµ‹è¯•åŸºæœ¬æ—¥å¿—æ€§èƒ½
        print("\n1. åŸºæœ¬æ—¥å¿—æ€§èƒ½æµ‹è¯•...")
        basic_log_times = []

        for i in range(100):
            start_time = time.perf_counter()
            service._log_info(f"æµ‹è¯•æ—¥å¿—æ¶ˆæ¯ {i}", test_data=f"test_value_{i}")
            end_time = time.perf_counter()

            log_time = (end_time - start_time) * 1000 * 1000  # è½¬æ¢ä¸ºå¾®ç§’
            basic_log_times.append(log_time)

        avg_basic_time = sum(basic_log_times) / len(basic_log_times)
        print(f"   å¹³å‡åŸºæœ¬æ—¥å¿—æ—¶é—´: {avg_basic_time:.2f}Î¼s")

        # æµ‹è¯•æ“ä½œæ—¥å¿—æ€§èƒ½
        print("\n2. æ“ä½œæ—¥å¿—æ€§èƒ½æµ‹è¯•...")
        operation_log_times = []

        for i in range(50):
            start_time = time.perf_counter()
            service._log_operation_start(f"æµ‹è¯•æ“ä½œ {i}", user_id=f"user_{i}")
            service._log_operation_success(f"æµ‹è¯•æ“ä½œ {i}", duration_ms=100 + i)
            end_time = time.perf_counter()

            log_time = (end_time - start_time) * 1000
            operation_log_times.append(log_time)

        avg_operation_time = sum(operation_log_times) / len(operation_log_times)
        print(f"   å¹³å‡æ“ä½œæ—¥å¿—æ—¶é—´: {avg_operation_time:.2f}ms")

        # æµ‹è¯•é”™è¯¯æ—¥å¿—æ€§èƒ½
        print("\n3. é”™è¯¯æ—¥å¿—æ€§èƒ½æµ‹è¯•...")
        error_log_times = []

        test_exception = Exception("æµ‹è¯•å¼‚å¸¸")

        for i in range(30):
            start_time = time.perf_counter()
            service._log_error(f"æµ‹è¯•é”™è¯¯æ—¥å¿— {i}", error=test_exception)
            end_time = time.perf_counter()

            log_time = (end_time - start_time) * 1000
            error_log_times.append(log_time)

        avg_error_time = sum(error_log_times) / len(error_log_times)
        print(f"   å¹³å‡é”™è¯¯æ—¥å¿—æ—¶é—´: {avg_error_time:.2f}ms")

        # æ€§èƒ½è¯„ä¼°
        print(f"\n=== æ—¥å¿—æ€§èƒ½è¯„ä¼° ===")
        if avg_basic_time > 1000:  # 1msé˜ˆå€¼
            print(f"âš ï¸ åŸºæœ¬æ—¥å¿—æ€§èƒ½è¾ƒæ…¢: {avg_basic_time:.2f}Î¼s")
        else:
            print(f"âœ… åŸºæœ¬æ—¥å¿—æ€§èƒ½è‰¯å¥½: {avg_basic_time:.2f}Î¼s")

        if avg_operation_time > 5:  # 5msé˜ˆå€¼
            print(f"âš ï¸ æ“ä½œæ—¥å¿—æ€§èƒ½è¾ƒæ…¢: {avg_operation_time:.2f}ms")
        else:
            print(f"âœ… æ“ä½œæ—¥å¿—æ€§èƒ½è‰¯å¥½: {avg_operation_time:.2f}ms")

        return {
            "basic_log_avg_us": avg_basic_time,
            "operation_log_avg_ms": avg_operation_time,
            "error_log_avg_ms": avg_error_time
        }

    def test_logging_decorator_performance(self):
        """æµ‹è¯•æ—¥å¿—è£…é¥°å™¨æ€§èƒ½"""
        print("\n=== æ—¥å¿—è£…é¥°å™¨æ€§èƒ½æµ‹è¯• ===")

        from src.services.logging_config import operation_logger, performance_logger

        class TestService(BaseService):
            @operation_logger("æµ‹è¯•æ–¹æ³•", log_args=True, log_result=True)
            def test_method(self, data: Dict[str, Any]) -> Dict[str, Any]:
                # æ¨¡æ‹Ÿä¸€äº›å¤„ç†
                result = {"processed": True, "data": data}
                return result

            @performance_logger("æ€§èƒ½æµ‹è¯•æ–¹æ³•")
            def performance_method(self, size: int) -> int:
                # æ¨¡æ‹Ÿä¸€äº›è®¡ç®—
                return sum(range(size))

        service = TestService()

        # æµ‹è¯•æ“ä½œæ—¥å¿—è£…é¥°å™¨
        print("\n1. æ“ä½œæ—¥å¿—è£…é¥°å™¨æ€§èƒ½æµ‹è¯•...")
        decorator_times = []

        test_data = {"key": "value", "number": 42}

        for i in range(50):
            start_time = time.perf_counter()
            result = service.test_method(test_data)
            end_time = time.perf_counter()

            decorator_time = (end_time - start_time) * 1000
            decorator_times.append(decorator_time)

        avg_decorator_time = sum(decorator_times) / len(decorator_times)
        print(f"   å¹³å‡è£…é¥°å™¨æ‰§è¡Œæ—¶é—´: {avg_decorator_time:.2f}ms")

        # æµ‹è¯•æ€§èƒ½æ—¥å¿—è£…é¥°å™¨
        print("\n2. æ€§èƒ½æ—¥å¿—è£…é¥°å™¨æµ‹è¯•...")
        perf_decorator_times = []

        for i in range(50):
            start_time = time.perf_counter()
            result = service.performance_method(1000)
            end_time = time.perf_counter()

            decorator_time = (end_time - start_time) * 1000
            perf_decorator_times.append(decorator_time)

        avg_perf_time = sum(perf_decorator_times) / len(perf_decorator_times)
        print(f"   å¹³å‡æ€§èƒ½è£…é¥°å™¨æ—¶é—´: {avg_perf_time:.2f}ms")

        # æ€§èƒ½è¯„ä¼°
        print(f"\n=== è£…é¥°å™¨æ€§èƒ½è¯„ä¼° ===")
        if avg_decorator_time > 10:  # 10msé˜ˆå€¼
            print(f"âš ï¸ æ“ä½œæ—¥å¿—è£…é¥°å™¨æ€§èƒ½è¾ƒæ…¢: {avg_decorator_time:.2f}ms")
        else:
            print(f"âœ… æ“ä½œæ—¥å¿—è£…é¥°å™¨æ€§èƒ½è‰¯å¥½: {avg_decorator_time:.2f}ms")

        if avg_perf_time > 5:  # 5msé˜ˆå€¼
            print(f"âš ï¸ æ€§èƒ½æ—¥å¿—è£…é¥°å™¨è¾ƒæ…¢: {avg_perf_time:.2f}ms")
        else:
            print(f"âœ… æ€§èƒ½æ—¥å¿—è£…é¥°å™¨è‰¯å¥½: {avg_perf_time:.2f}ms")

        return {
            "operation_decorator_ms": avg_decorator_time,
            "performance_decorator_ms": avg_perf_time
        }


class TestExceptionHandlingPerformance(PerformanceTestBase):
    """å¼‚å¸¸å¤„ç†æ€§èƒ½æµ‹è¯•"""

    def test_exception_creation_performance(self):
        """æµ‹è¯•å¼‚å¸¸åˆ›å»ºæ€§èƒ½"""
        print("\n=== å¼‚å¸¸åˆ›å»ºæ€§èƒ½æµ‹è¯• ===")

        from src.services.exceptions import (
            BusinessException,
            ValidationException,
            ResourceNotFoundException
        )

        # æµ‹è¯•ä¸åŒå¼‚å¸¸ç±»å‹çš„åˆ›å»ºæ€§èƒ½
        exception_types = [
            (BusinessException, "error_code", "message"),
            (ValidationException, "field", "value", "message"),
            (ResourceNotFoundException, "resource", "id")
        ]

        results = {}

        for exception_type in exception_types:
            print(f"\næµ‹è¯• {exception_type.__name__} åˆ›å»ºæ€§èƒ½...")

            creation_times = []
            args = exception_type[1:]  # è·å–æ„é€ å‡½æ•°å‚æ•°

            for i in range(1000):
                start_time = time.perf_counter()
                exception = exception_type(*args)
                end_time = time.perf_counter()

                creation_time = (end_time - start_time) * 1000 * 1000  # å¾®ç§’
                creation_times.append(creation_time)

                del exception

            avg_time = sum(creation_times) / len(creation_times)
            results[exception_type.__name__] = avg_time
            print(f"   å¹³å‡åˆ›å»ºæ—¶é—´: {avg_time:.2f}Î¼s")

        # æ€§èƒ½è¯„ä¼°
        print(f"\n=== å¼‚å¸¸åˆ›å»ºæ€§èƒ½è¯„ä¼° ===")
        for exc_name, avg_time in results.items():
            if avg_time > 100:  # 100å¾®ç§’é˜ˆå€¼
                print(f"âš ï¸ {exc_name} åˆ›å»ºè¾ƒæ…¢: {avg_time:.2f}Î¼s")
            else:
                print(f"âœ… {exc_name} åˆ›å»ºæ€§èƒ½è‰¯å¥½: {avg_time:.2f}Î¼s")

        return results

    def test_exception_handling_overhead(self):
        """æµ‹è¯•å¼‚å¸¸å¤„ç†å¼€é”€"""
        print("\n=== å¼‚å¸¸å¤„ç†å¼€é”€æµ‹è¯• ===")

        service = UserService()

        # æµ‹è¯•æ­£å¸¸æ‰§è¡Œæ€§èƒ½
        normal_times = []

        for i in range(100):
            start_time = time.perf_counter()
            # æ­£å¸¸çš„éªŒè¯è°ƒç”¨
            service.validate_required_fields(
                {"name": "test", "email": "test@example.com"},
                ["name", "email"]
            )
            end_time = time.perf_counter()

            normal_time = (end_time - start_time) * 1000
            normal_times.append(normal_time)

        avg_normal_time = sum(normal_times) / len(normal_times)

        # æµ‹è¯•å¼‚å¸¸å¤„ç†æ€§èƒ½
        exception_times = []

        for i in range(100):
            start_time = time.perf_counter()
            try:
                # ä¼šæŠ›å‡ºå¼‚å¸¸çš„è°ƒç”¨
                service.validate_required_fields(
                    {"name": "test"},  # ç¼ºå°‘emailå­—æ®µ
                    ["name", "email"]
                )
            except Exception:
                pass
            end_time = time.perf_counter()

            exception_time = (end_time - start_time) * 1000
            exception_times.append(exception_time)

        avg_exception_time = sum(exception_times) / len(exception_times)

        print(f"æ­£å¸¸æ‰§è¡Œå¹³å‡æ—¶é—´: {avg_normal_time:.2f}ms")
        print(f"å¼‚å¸¸å¤„ç†å¹³å‡æ—¶é—´: {avg_exception_time:.2f}ms")
        print(f"å¼‚å¸¸å¤„ç†å¼€é”€: {(avg_exception_time / avg_normal_time - 1) * 100:.1f}%")

        if avg_exception_time > avg_normal_time * 5:
            print("âš ï¸ å¼‚å¸¸å¤„ç†å¼€é”€è¾ƒå¤§ï¼Œéœ€è¦ä¼˜åŒ–")
        else:
            print("âœ… å¼‚å¸¸å¤„ç†å¼€é”€åœ¨å¯æ¥å—èŒƒå›´å†…")

        return {
            "normal_time_ms": avg_normal_time,
            "exception_time_ms": avg_exception_time,
            "overhead_percent": (avg_exception_time / avg_normal_time - 1) * 100
        }


class TestConcurrentAccessPerformance(PerformanceTestBase):
    """å¹¶å‘è®¿é—®æ€§èƒ½æµ‹è¯•"""

    def test_concurrent_service_creation(self):
        """æµ‹è¯•å¹¶å‘æœåŠ¡åˆ›å»ºæ€§èƒ½"""
        print("\n=== å¹¶å‘æœåŠ¡åˆ›å»ºæ€§èƒ½æµ‹è¯• ===")

        def create_service_thread(service_class, results_list, thread_id):
            """åˆ›å»ºæœåŠ¡çš„çº¿ç¨‹å‡½æ•°"""
            thread_times = []

            for i in range(10):
                start_time = time.perf_counter()
                service = service_class()
                end_time = time.perf_counter()

                creation_time = (end_time - start_time) * 1000
                thread_times.append(creation_time)

                del service

            results_list.append({
                "thread_id": thread_id,
                "times": thread_times,
                "avg_time": sum(thread_times) / len(thread_times)
            })

        # æµ‹è¯•ä¸åŒæœåŠ¡çš„å¹¶å‘åˆ›å»º
        services_to_test = [
            (UserService, "UserService"),
            (TaskService, "TaskService"),
            (FocusService, "FocusService")
        ]

        for service_class, service_name in services_to_test:
            print(f"\næµ‹è¯• {service_name} å¹¶å‘åˆ›å»ºæ€§èƒ½...")

            threads = []
            results = []

            # åˆ›å»º10ä¸ªå¹¶å‘çº¿ç¨‹
            for i in range(10):
                thread = threading.Thread(
                    target=create_service_thread,
                    args=(service_class, results, i)
                )
                threads.append(thread)

            # å¯åŠ¨æ‰€æœ‰çº¿ç¨‹
            start_time = time.perf_counter()
            for thread in threads:
                thread.start()

            # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
            for thread in threads:
                thread.join()
            end_time = time.perf_counter()

            total_time = (end_time - start_time) * 1000
            total_operations = sum(len(r["times"]) for r in results)
            avg_thread_time = sum(r["avg_time"] for r in results) / len(results)

            print(f"   æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}ms")
            print(f"   æ€»æ“ä½œæ•°: {total_operations}")
            print(f"   æ¯çº¿ç¨‹å¹³å‡æ—¶é—´: {avg_thread_time:.2f}ms")
            print(f"   ååé‡: {total_operations / (total_time / 1000):.2f} æ“ä½œ/ç§’")

    def test_concurrent_logging(self):
        """æµ‹è¯•å¹¶å‘æ—¥å¿—è®°å½•æ€§èƒ½"""
        print("\n=== å¹¶å‘æ—¥å¿—è®°å½•æ€§èƒ½æµ‹è¯• ===")

        os.environ['SERVICE_LOG_LEVEL'] = 'INFO'
        os.environ['SERVICE_LOG_CONSOLE'] = 'false'
        os.environ['SERVICE_LOG_FILE'] = 'false'

        def logging_thread(service, thread_id, results):
            """æ—¥å¿—è®°å½•çº¿ç¨‹å‡½æ•°"""
            thread_times = []

            for i in range(50):
                start_time = time.perf_counter()
                service._log_info(f"çº¿ç¨‹{thread_id}æ—¥å¿—{i}", thread_id=thread_id, iteration=i)
                end_time = time.perf_counter()

                log_time = (end_time - start_time) * 1000 * 1000  # å¾®ç§’
                thread_times.append(log_time)

            results.append({
                "thread_id": thread_id,
                "avg_time_us": sum(thread_times) / len(thread_times),
                "times": thread_times
            })

        service = UserService()
        threads = []
        results = []

        # åˆ›å»º20ä¸ªå¹¶å‘æ—¥å¿—çº¿ç¨‹
        for i in range(20):
            thread = threading.Thread(
                target=logging_thread,
                args=(service, i, results)
            )
            threads.append(thread)

        # å¯åŠ¨æ‰€æœ‰çº¿ç¨‹
        start_time = time.perf_counter()
        for thread in threads:
            thread.start()

        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        end_time = time.perf_counter()

        total_time = (end_time - start_time) * 1000
        total_logs = sum(len(r["times"]) for r in results)
        avg_thread_time = sum(r["avg_time_us"] for r in results) / len(results)

        print(f"   æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}ms")
        print(f"   æ€»æ—¥å¿—æ•°: {total_logs}")
        print(f"   æ¯çº¿ç¨‹å¹³å‡æ—¶é—´: {avg_thread_time:.2f}Î¼s")
        print(f"   æ—¥å¿—ååé‡: {total_logs / (total_time / 1000):.2f} æ—¥å¿—/ç§’")

        # æ€§èƒ½è¯„ä¼°
        if total_logs / (total_time / 1000) < 1000:  # 1000æ—¥å¿—/ç§’é˜ˆå€¼
            print("âš ï¸ å¹¶å‘æ—¥å¿—æ€§èƒ½è¾ƒä½")
        else:
            print("âœ… å¹¶å‘æ—¥å¿—æ€§èƒ½è‰¯å¥½")


class TestMemoryUsageOptimization(PerformanceTestBase):
    """å†…å­˜ä½¿ç”¨ä¼˜åŒ–æµ‹è¯•"""

    def test_service_memory_leaks(self):
        """æµ‹è¯•æœåŠ¡å†…å­˜æ³„æ¼"""
        print("\n=== æœåŠ¡å†…å­˜æ³„æ¼æµ‹è¯• ===")

        services_to_test = [UserService, TaskService, FocusService]

        for service_class in services_to_test:
            print(f"\næµ‹è¯• {service_class.__name__} å†…å­˜ä½¿ç”¨...")

            initial_memory = self.measure_memory_usage()

            # åˆ›å»ºå¤§é‡æœåŠ¡å®ä¾‹
            services = []
            for i in range(100):
                service = service_class()
                services.append(service)

            peak_memory = self.measure_memory_usage()

            # åˆ é™¤æœåŠ¡å®ä¾‹
            del services

            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()

            final_memory = self.measure_memory_usage()

            memory_increase = peak_memory["rss"] - initial_memory["rss"]
            memory_leak = final_memory["rss"] - initial_memory["rss"]

            print(f"   åˆå§‹å†…å­˜: {initial_memory['rss']:.2f}MB")
            print(f"   å³°å€¼å†…å­˜: {peak_memory['rss']:.2f}MB")
            print(f"   æœ€ç»ˆå†…å­˜: {final_memory['rss']:.2f}MB")
            print(f"   å†…å­˜å¢é•¿: {memory_increase:.2f}MB")
            print(f"   å†…å­˜æ³„æ¼: {memory_leak:.2f}MB")

            if memory_leak > 10:  # 10MBé˜ˆå€¼
                print(f"âš ï¸ {service_class.__name__} å¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼")
            else:
                print(f"âœ… {service_class.__name__} å†…å­˜ä½¿ç”¨æ­£å¸¸")

    def test_logging_memory_impact(self):
        """æµ‹è¯•æ—¥å¿—ç³»ç»Ÿå¯¹å†…å­˜çš„å½±å“"""
        print("\n=== æ—¥å¿—ç³»ç»Ÿå†…å­˜å½±å“æµ‹è¯• ===")

        os.environ['SERVICE_LOG_LEVEL'] = 'DEBUG'
        os.environ['SERVICE_LOG_CONSOLE'] = 'false'
        os.environ['SERVICE_LOG_FILE'] = 'false'

        service = UserService()

        # æµ‹è¯•å¤§é‡æ—¥å¿—è®°å½•çš„å†…å­˜å½±å“
        initial_memory = self.measure_memory_usage()

        # è®°å½•å¤§é‡æ—¥å¿—
        for i in range(1000):
            service._log_info(f"å†…å­˜æµ‹è¯•æ—¥å¿— {i}",
                            iteration=i,
                            large_data="x" * 100)  # åŒ…å«ä¸€äº›æ•°æ®

        after_logging_memory = self.measure_memory_usage()

        # æµ‹è¯•å¼‚å¸¸æ—¥å¿—çš„å†…å­˜å½±å“
        test_exception = Exception("å†…å­˜æµ‹è¯•å¼‚å¸¸")
        for i in range(100):
            service._log_error(f"å†…å­˜æµ‹è¯•é”™è¯¯ {i}",
                            error=test_exception,
                            iteration=i)

        after_error_memory = self.measure_memory_usage()

        # è®¡ç®—å†…å­˜å¢é•¿
        logging_growth = after_logging_memory["rss"] - initial_memory["rss"]
        error_growth = after_error_memory["rss"] - after_logging_memory["rss"]

        print(f"   åˆå§‹å†…å­˜: {initial_memory['rss']:.2f}MB")
        print(f"   å¤§é‡æ—¥å¿—å: {after_logging_memory['rss']:.2f}MB")
        print(f"   å¤§é‡é”™è¯¯å: {after_error_memory['rss']:.2f}MB")
        print(f"   æ—¥å¿—å†…å­˜å¢é•¿: {logging_growth:.2f}MB")
        print(f"   é”™è¯¯å†…å­˜å¢é•¿: {error_growth:.2f}MB")

        if logging_growth > 20:  # 20MBé˜ˆå€¼
            print("âš ï¸ æ—¥å¿—ç³»ç»Ÿå†…å­˜ä½¿ç”¨è¾ƒé«˜")
        else:
            print("âœ… æ—¥å¿—ç³»ç»Ÿå†…å­˜ä½¿ç”¨æ­£å¸¸")


class TestOverallSystemPerformance(PerformanceTestBase):
    """æ•´ä½“ç³»ç»Ÿæ€§èƒ½æµ‹è¯•"""

    def test_end_to_end_performance(self):
        """ç«¯åˆ°ç«¯æ€§èƒ½æµ‹è¯•"""
        print("\n=== ç«¯åˆ°ç«¯æ€§èƒ½æµ‹è¯• ===")

        # æ¨¡æ‹Ÿå®Œæ•´çš„ä¸šåŠ¡æµç¨‹
        service = UserService()

        # æµ‹è¯•å®Œæ•´çš„ç”¨æˆ·ç®¡ç†æµç¨‹
        workflows = [
            "ç”¨æˆ·æ³¨å†Œæµç¨‹",
            "ç”¨æˆ·èµ„æ–™æ›´æ–°æµç¨‹",
            "ç”¨æˆ·è®¾ç½®ç®¡ç†æµç¨‹",
            "ç§¯åˆ†æ“ä½œæµç¨‹"
        ]

        workflow_times = {}

        for workflow in workflows:
            print(f"\næµ‹è¯• {workflow}...")

            workflow_times_list = []

            for i in range(20):
                start_time = time.perf_counter()

                # æ¨¡æ‹Ÿä¸šåŠ¡æµç¨‹
                if workflow == "ç”¨æˆ·æ³¨å†Œæµç¨‹":
                    # æ¨¡æ‹Ÿç”¨æˆ·æ³¨å†Œ
                    service._log_operation_start("ç”¨æˆ·æ³¨å†Œ", user_id=f"user_{i}")
                    # æ¨¡æ‹Ÿä¸€äº›å¤„ç†
                    result = {"user_id": f"user_{i}", "status": "created"}
                    service._log_operation_success("ç”¨æˆ·æ³¨å†Œ", user_id=f"user_{i}")

                elif workflow == "ç”¨æˆ·èµ„æ–™æ›´æ–°æµç¨‹":
                    # æ¨¡æ‹Ÿèµ„æ–™æ›´æ–°
                    profile_data = {
                        "display_name": f"ç”¨æˆ·{i}",
                        "avatar_url": f"https://avatar{i}.jpg"
                    }
                    service._log_operation_start("æ›´æ–°èµ„æ–™", user_id=f"user_{i}")
                    result = {"updated": True, "data": profile_data}
                    service._log_operation_success("æ›´æ–°èµ„æ–™", user_id=f"user_{i}")

                elif workflow == "ç”¨æˆ·è®¾ç½®ç®¡ç†æµç¨‹":
                    # æ¨¡æ‹Ÿè®¾ç½®ç®¡ç†
                    settings_data = {
                        "notification_enabled": True,
                        "theme": "dark",
                        "language": "zh-CN"
                    }
                    service._log_operation_start("æ›´æ–°è®¾ç½®", user_id=f"user_{i}")
                    result = {"updated": True, "settings": settings_data}
                    service._log_operation_success("æ›´æ–°è®¾ç½®", user_id=f"user_{i}")

                elif workflow == "ç§¯åˆ†æ“ä½œæµç¨‹":
                    # æ¨¡æ‹Ÿç§¯åˆ†æ“ä½œ
                    service._log_operation_start("æ·»åŠ ç§¯åˆ†", user_id=f"user_{i}")
                    result = {"points_added": 50, "new_balance": 150}
                    service._log_operation_success("æ·»åŠ ç§¯åˆ†", user_id=f"user_{i}")

                end_time = time.perf_counter()
                workflow_time = (end_time - start_time) * 1000
                workflow_times_list.append(workflow_time)

            avg_time = sum(workflow_times_list) / len(workflow_times_list)
            workflow_times[workflow] = avg_time

            print(f"   å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_time:.2f}ms")

        # æ€§èƒ½è¯„ä¼°
        print(f"\n=== ç«¯åˆ°ç«¯æ€§èƒ½è¯„ä¼° ===")
        total_avg_time = sum(workflow_times.values()) / len(workflow_times)
        print(f"æ•´ä½“å¹³å‡æ‰§è¡Œæ—¶é—´: {total_avg_time:.2f}ms")

        for workflow, avg_time in workflow_times.items():
            if avg_time > 50:  # 50msé˜ˆå€¼
                print(f"âš ï¸ {workflow} æ€§èƒ½è¾ƒæ…¢: {avg_time:.2f}ms")
            else:
                print(f"âœ… {workflow} æ€§èƒ½è‰¯å¥½: {avg_time:.2f}ms")

        return workflow_times

    def generate_performance_report(self, all_test_results: Dict[str, Any]):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*50)
        print("æ€§èƒ½æµ‹è¯•ç»¼åˆæŠ¥å‘Š")
        print("="*50)

        # è¿™é‡Œå¯ä»¥ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š
        # åŒ…æ‹¬å„ä¸ªæµ‹è¯•çš„ç»“æœæ±‡æ€»ã€æ€§èƒ½ç“¶é¢ˆåˆ†æã€ä¼˜åŒ–å»ºè®®ç­‰

        print("\nğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
        for test_name, results in all_test_results.items():
            print(f"\nğŸ” {test_name}:")
            if isinstance(results, dict):
                for key, value in results.items():
                    if isinstance(value, (int, float)):
                        print(f"   {key}: {value:.2f}")
                    else:
                        print(f"   {key}: {value}")

        print(f"\nğŸ¯ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
        print("1. ç›‘æ§æ—¥å¿—ç³»ç»Ÿæ€§èƒ½ï¼Œé¿å…åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨DEBUGçº§åˆ«")
        print("2. ä¼˜åŒ–å¼‚å¸¸å¤„ç†ï¼Œå‡å°‘ä¸å¿…è¦çš„å¼‚å¸¸åˆ›å»º")
        print("3. è€ƒè™‘ä½¿ç”¨å¯¹è±¡æ± æ¥é‡ç”¨æœåŠ¡å®ä¾‹")
        print("4. å®šæœŸè¿›è¡Œå†…å­˜æ³„æ¼æ£€æµ‹")
        print("5. ç›‘æ§å¹¶å‘è®¿é—®æ€§èƒ½ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨")


@pytest.fixture
def performance_test_results():
    """å­˜å‚¨æ‰€æœ‰æµ‹è¯•ç»“æœçš„fixture"""
    return {}


class TestServicePerformanceComplete:
    """å®Œæ•´çš„æœåŠ¡å±‚æ€§èƒ½æµ‹è¯•å¥—ä»¶"""

    def test_all_performance_tests(self, performance_test_results):
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æœåŠ¡å±‚å®Œæ•´æ€§èƒ½æµ‹è¯•")
        print("="*60)

        # 1. æœåŠ¡åˆå§‹åŒ–æ€§èƒ½æµ‹è¯•
        init_test = TestServiceInitializationPerformance()
        init_results = init_test.test_service_initialization_performance()
        init_test.test_service_factory_performance()
        performance_test_results["initialization"] = init_results

        # 2. æ—¥å¿—ç³»ç»Ÿæ€§èƒ½æµ‹è¯•
        logging_test = TestLoggingSystemPerformance()
        logging_results = logging_test.test_logging_performance()
        decorator_results = logging_test.test_logging_decorator_performance()
        performance_test_results["logging"] = {**logging_results, **decorator_results}

        # 3. å¼‚å¸¸å¤„ç†æ€§èƒ½æµ‹è¯•
        exception_test = TestExceptionHandlingPerformance()
        exception_results = exception_test.test_exception_creation_performance()
        handling_results = exception_test.test_exception_handling_overhead()
        performance_test_results["exceptions"] = {**exception_results, **handling_results}

        # 4. å¹¶å‘è®¿é—®æ€§èƒ½æµ‹è¯•
        concurrent_test = TestConcurrentAccessPerformance()
        concurrent_test.test_concurrent_service_creation()
        concurrent_test.test_concurrent_logging()

        # 5. å†…å­˜ä½¿ç”¨æµ‹è¯•
        memory_test = TestMemoryUsageOptimization()
        memory_test.test_service_memory_leaks()
        memory_test.test_logging_memory_impact()

        # 6. ç«¯åˆ°ç«¯æ€§èƒ½æµ‹è¯•
        e2e_test = TestOverallSystemPerformance()
        e2e_results = e2e_test.test_end_to_end_performance()
        performance_test_results["end_to_end"] = e2e_results

        # 7. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        e2e_test.generate_performance_report(performance_test_results)

        print("\nâœ… æ‰€æœ‰æ€§èƒ½æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œæ€§èƒ½æµ‹è¯•
    suite = TestServicePerformanceComplete()
    results = {}
    suite.test_all_performance_tests(results)