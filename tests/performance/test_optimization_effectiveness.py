"""
æ€§èƒ½ä¼˜åŒ–æ•ˆæœæµ‹è¯•

è¯¥æ¨¡å—æµ‹è¯•å„ç§æ€§èƒ½ä¼˜åŒ–ç­–ç•¥çš„å®é™…æ•ˆæœï¼ŒåŒ…æ‹¬ï¼š
1. å¼‚å¸¸å¤„ç†ä¼˜åŒ–
2. æ—¥å¿—è®°å½•ä¼˜åŒ–
3. å¿«é€ŸéªŒè¯ä¼˜åŒ–
4. æ‰¹é‡æ“ä½œä¼˜åŒ–
"""

import time
import os
import sys
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from src.services.base import BaseService
from src.services.base_optimized import OptimizedBaseService
from src.services.exceptions import ValidationException, ResourceNotFoundException
from src.services.performance_optimization import (
    FastExceptionHandler,
    fast_validate_required,
    performance_monitor,
    conditional_logging
)


class PerformanceComparisonTest:
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•ç±»"""

    def __init__(self):
        self.iterations = 1000
        self.results = {}

    def measure_execution_time(self, func, *args, **kwargs) -> tuple:
        """
        æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´

        Returns:
            (æ‰§è¡Œç»“æœ, æ‰§è¡Œæ—¶é—´æ¯«ç§’)
        """
        start_time = time.perf_counter()
        try:
            result = func(*args, **kwargs)
            return result, (time.perf_counter() - start_time) * 1000
        except Exception as e:
            return e, (time.perf_counter() - start_time) * 1000

    def test_exception_creation_performance(self):
        """æµ‹è¯•å¼‚å¸¸åˆ›å»ºæ€§èƒ½å¯¹æ¯”"""
        print("\n=== å¼‚å¸¸åˆ›å»ºæ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")

        # åŸå§‹å¼‚å¸¸åˆ›å»ºæ–¹å¼
        def create_original_exception():
            return ValidationException(
                field="test_field",
                value="invalid_value",
                message="éªŒè¯å¤±è´¥",
                user_message="ç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯"
            )

        # ä¼˜åŒ–å¼‚å¸¸åˆ›å»ºæ–¹å¼
        def create_optimized_exception():
            handler = FastExceptionHandler()
            return handler.fast_validation_error(
                field="test_field",
                value="invalid_value",
                user_message="ç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯"
            )

        # æµ‹è¯•åŸå§‹æ–¹å¼
        original_times = []
        for _ in range(self.iterations):
            _, exec_time = self.measure_execution_time(create_original_exception)
            original_times.append(exec_time)

        # æµ‹è¯•ä¼˜åŒ–æ–¹å¼
        optimized_times = []
        for _ in range(self.iterations):
            _, exec_time = self.measure_execution_time(create_optimized_exception)
            optimized_times.append(exec_time)

        avg_original = sum(original_times) / len(original_times)
        avg_optimized = sum(optimized_times) / len(optimized_times)
        improvement = ((avg_original - avg_optimized) / avg_original) * 100

        print(f"åŸå§‹å¼‚å¸¸åˆ›å»ºå¹³å‡æ—¶é—´: {avg_original:.3f}Î¼s")
        print(f"ä¼˜åŒ–å¼‚å¸¸åˆ›å»ºå¹³å‡æ—¶é—´: {avg_optimized:.3f}Î¼s")
        print(f"æ€§èƒ½æå‡: {improvement:.1f}%")

        self.results["exception_creation"] = {
            "original": avg_original,
            "optimized": avg_optimized,
            "improvement": improvement
        }

    def test_validation_performance(self):
        """æµ‹è¯•éªŒè¯æ€§èƒ½å¯¹æ¯”"""
        print("\n=== éªŒè¯æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")

        test_data = {"name": "test", "value": 123}
        required_fields = ["name", "value", "type"]  # ç¼ºå°‘typeå­—æ®µ

        # åŸå§‹éªŒè¯æ–¹å¼ï¼ˆæ¨¡æ‹ŸBaseServiceä¸­çš„æ–¹å¼ï¼‰
        def original_validation():
            try:
                for field in required_fields:
                    if field not in test_data or test_data[field] is None:
                        raise ValidationException(
                            field=field,
                            value=test_data.get(field),
                            message=f"Missing required field: {field}",
                            user_message=f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}"
                        )
                return True
            except Exception:
                return False

        # ä¼˜åŒ–éªŒè¯æ–¹å¼
        def optimized_validation():
            try:
                fast_validate_required(test_data, required_fields)
                return True
            except Exception:
                return False

        # æµ‹è¯•åŸå§‹æ–¹å¼
        original_times = []
        for _ in range(self.iterations):
            _, exec_time = self.measure_execution_time(original_validation)
            original_times.append(exec_time)

        # æµ‹è¯•ä¼˜åŒ–æ–¹å¼
        optimized_times = []
        for _ in range(self.iterations):
            _, exec_time = self.measure_execution_time(optimized_validation)
            optimized_times.append(exec_time)

        avg_original = sum(original_times) / len(original_times)
        avg_optimized = sum(optimized_times) / len(optimized_times)
        improvement = ((avg_original - avg_optimized) / avg_original) * 100

        print(f"åŸå§‹éªŒè¯å¹³å‡æ—¶é—´: {avg_original:.3f}Î¼s")
        print(f"ä¼˜åŒ–éªŒè¯å¹³å‡æ—¶é—´: {avg_optimized:.3f}Î¼s")
        print(f"æ€§èƒ½æå‡: {improvement:.1f}%")

        self.results["validation"] = {
            "original": avg_original,
            "optimized": avg_optimized,
            "improvement": improvement
        }

    def test_service_instantiation_performance(self):
        """æµ‹è¯•æœåŠ¡å®ä¾‹åŒ–æ€§èƒ½å¯¹æ¯”"""
        print("\n=== æœåŠ¡å®ä¾‹åŒ–æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")

        # æµ‹è¯•åŸå§‹BaseService
        def create_original_service():
            return BaseService()

        # æµ‹è¯•ä¼˜åŒ–ç‰ˆBaseService
        def create_optimized_service():
            return OptimizedBaseService(enable_detailed_logging=False)

        # æµ‹è¯•åŸå§‹æœåŠ¡
        original_times = []
        for _ in range(100):  # å‡å°‘è¿­ä»£æ¬¡æ•°ï¼Œå› ä¸ºæœåŠ¡åˆ›å»ºå¼€é”€è¾ƒå¤§
            _, exec_time = self.measure_execution_time(create_original_service)
            original_times.append(exec_time)

        # æµ‹è¯•ä¼˜åŒ–æœåŠ¡
        optimized_times = []
        for _ in range(100):
            _, exec_time = self.measure_execution_time(create_optimized_service)
            optimized_times.append(exec_time)

        avg_original = sum(original_times) / len(original_times)
        avg_optimized = sum(optimized_times) / len(optimized_times)
        improvement = ((avg_original - avg_optimized) / avg_original) * 100

        print(f"åŸå§‹æœåŠ¡å®ä¾‹åŒ–å¹³å‡æ—¶é—´: {avg_original:.3f}ms")
        print(f"ä¼˜åŒ–æœåŠ¡å®ä¾‹åŒ–å¹³å‡æ—¶é—´: {avg_optimized:.3f}ms")
        print(f"æ€§èƒ½æå‡: {improvement:.1f}%")

        self.results["service_instantiation"] = {
            "original": avg_original,
            "optimized": avg_optimized,
            "improvement": improvement
        }

    def test_logging_performance(self):
        """æµ‹è¯•æ—¥å¿—æ€§èƒ½å¯¹æ¯”"""
        print("\n=== æ—¥å¿—æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")

        # åˆ›å»ºæµ‹è¯•æœåŠ¡
        original_service = BaseService()
        optimized_service = OptimizedBaseService(enable_detailed_logging=False)

        # æµ‹è¯•åŸå§‹æœåŠ¡æ—¥å¿—
        def original_logging():
            original_service._log_info("æµ‹è¯•æ—¥å¿—æ¶ˆæ¯", extra_data={"key": "value"})

        # æµ‹è¯•ä¼˜åŒ–æœåŠ¡æ—¥å¿—
        def optimized_logging():
            optimized_service._log_info("æµ‹è¯•æ—¥å¿—æ¶ˆæ¯", extra_data={"key": "value"})

        # æµ‹è¯•åŸå§‹æ—¥å¿—
        original_times = []
        for _ in range(self.iterations):
            _, exec_time = self.measure_execution_time(original_logging)
            original_times.append(exec_time)

        # æµ‹è¯•ä¼˜åŒ–æ—¥å¿—
        optimized_times = []
        for _ in range(self.iterations):
            _, exec_time = self.measure_execution_time(optimized_logging)
            optimized_times.append(exec_time)

        avg_original = sum(original_times) / len(original_times)
        avg_optimized = sum(optimized_times) / len(optimized_times)
        improvement = ((avg_original - avg_optimized) / avg_original) * 100

        print(f"åŸå§‹æ—¥å¿—è®°å½•å¹³å‡æ—¶é—´: {avg_original:.3f}Î¼s")
        print(f"ä¼˜åŒ–æ—¥å¿—è®°å½•å¹³å‡æ—¶é—´: {avg_optimized:.3f}Î¼s")
        print(f"æ€§èƒ½æå‡: {improvement:.1f}%")

        self.results["logging"] = {
            "original": avg_original,
            "optimized": avg_optimized,
            "improvement": improvement
        }

    def test_error_handling_performance(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†æ€§èƒ½å¯¹æ¯”"""
        print("\n=== é”™è¯¯å¤„ç†æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")

        # åˆ›å»ºæµ‹è¯•æœåŠ¡
        original_service = BaseService()
        optimized_service = OptimizedBaseService(enable_detailed_logging=False)

        test_error = Exception("æµ‹è¯•é”™è¯¯")

        # æµ‹è¯•åŸå§‹é”™è¯¯å¤„ç†
        def original_error_handling():
            try:
                original_service._handle_repository_error(test_error, "test_operation")
            except Exception:
                pass  # å¿½ç•¥å¼‚å¸¸ï¼Œåªæµ‹è¯•å¤„ç†æ€§èƒ½

        # æµ‹è¯•ä¼˜åŒ–é”™è¯¯å¤„ç†
        def optimized_error_handling():
            try:
                optimized_service._handle_repository_error_fast(test_error, "test_operation")
            except Exception:
                pass  # å¿½ç•¥å¼‚å¸¸ï¼Œåªæµ‹è¯•å¤„ç†æ€§èƒ½

        # æµ‹è¯•åŸå§‹é”™è¯¯å¤„ç†
        original_times = []
        for _ in range(100):  # å‡å°‘è¿­ä»£æ¬¡æ•°ï¼Œå› ä¸ºé”™è¯¯å¤„ç†å¼€é”€è¾ƒå¤§
            _, exec_time = self.measure_execution_time(original_error_handling)
            original_times.append(exec_time)

        # æµ‹è¯•ä¼˜åŒ–é”™è¯¯å¤„ç†
        optimized_times = []
        for _ in range(100):
            _, exec_time = self.measure_execution_time(optimized_error_handling)
            optimized_times.append(exec_time)

        avg_original = sum(original_times) / len(original_times)
        avg_optimized = sum(optimized_times) / len(optimized_times)
        improvement = ((avg_original - avg_optimized) / avg_original) * 100

        print(f"åŸå§‹é”™è¯¯å¤„ç†å¹³å‡æ—¶é—´: {avg_original:.3f}ms")
        print(f"ä¼˜åŒ–é”™è¯¯å¤„ç†å¹³å‡æ—¶é—´: {avg_optimized:.3f}ms")
        print(f"æ€§èƒ½æå‡: {improvement:.1f}%")

        self.results["error_handling"] = {
            "original": avg_original,
            "optimized": avg_optimized,
            "improvement": improvement
        }

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ€§èƒ½ä¼˜åŒ–æ•ˆæœæµ‹è¯•")
        print("="*60)

        self.test_exception_creation_performance()
        self.test_validation_performance()
        self.test_service_instantiation_performance()
        self.test_logging_performance()
        self.test_error_handling_performance()

        self._print_summary()

    def _print_summary(self):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "="*60)
        print("æ€§èƒ½ä¼˜åŒ–æ•ˆæœæ€»ç»“")
        print("="*60)

        total_improvement = 0
        test_count = 0

        for test_name, result in self.results.items():
            improvement = result["improvement"]
            if improvement > 0:
                print(f"âœ… {test_name}: æ€§èƒ½æå‡ {improvement:.1f}%")
                total_improvement += improvement
            else:
                print(f"âŒ {test_name}: æ€§èƒ½ä¸‹é™ {abs(improvement):.1f}%")
            test_count += 1

        if test_count > 0:
            avg_improvement = total_improvement / test_count
            print(f"\nğŸ“Š å¹³å‡æ€§èƒ½æå‡: {avg_improvement:.1f}%")

        # ä¼˜åŒ–å»ºè®®
        print("\nğŸ¯ ä¼˜åŒ–å»ºè®®:")
        if self.results.get("exception_handling", {}).get("improvement", 0) > 50:
            print("1. å¼‚å¸¸å¤„ç†ä¼˜åŒ–æ•ˆæœæ˜¾è‘—ï¼Œå»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨")
        if self.results.get("logging", {}).get("improvement", 0) > 30:
            print("2. æ—¥å¿—ä¼˜åŒ–æ•ˆæœæ˜æ˜¾ï¼Œå»ºè®®åœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹å¯ç”¨ä¼˜åŒ–æ¨¡å¼")
        if self.results.get("validation", {}).get("improvement", 0) > 20:
            print("3. éªŒè¯ä¼˜åŒ–æœ‰æ•ˆï¼Œå»ºè®®åœ¨æ•°æ®å¯†é›†å‹æ“ä½œä¸­ä½¿ç”¨å¿«é€ŸéªŒè¯")


def main():
    """ä¸»å‡½æ•°"""
    test = PerformanceComparisonTest()
    test.run_all_tests()
    print("\nâœ… æ€§èƒ½ä¼˜åŒ–æ•ˆæœæµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()