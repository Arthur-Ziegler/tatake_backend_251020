"""
æ€§èƒ½ä¼˜åŒ–æ•ˆæœéªŒè¯æµ‹è¯•

éªŒè¯å®æ–½çš„ä¼˜åŒ–ç­–ç•¥æ˜¯å¦æœ‰æ•ˆè§£å†³äº†å¼‚å¸¸å¤„ç†å¼€é”€è¿‡å¤§çš„é—®é¢˜ã€‚
"""

import os
import sys
import time
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from src.services.base import BaseService
from src.services.base_performance_optimized import PerformanceOptimizedBaseService
from src.services.optimized_exception_handler import get_optimized_exception_handler


class OptimizationResultTest:
    """ä¼˜åŒ–æ•ˆæœéªŒè¯æµ‹è¯•ç±»"""

    def __init__(self):
        self.iterations = 100
        self.test_results = {}

    def measure_time(self, func, *args, **kwargs) -> tuple:
        """æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´"""
        start_time = time.perf_counter()
        try:
            result = func(*args, **kwargs)
            return result, (time.perf_counter() - start_time) * 1000
        except Exception as e:
            return e, (time.perf_counter() - start_time) * 1000

    def test_exception_handling_overhead_comparison(self):
        """æµ‹è¯•å¼‚å¸¸å¤„ç†å¼€é”€å¯¹æ¯”"""
        print("\n=== å¼‚å¸¸å¤„ç†å¼€é”€å¯¹æ¯”æµ‹è¯• ===")

        # é…ç½®ç¯å¢ƒå˜é‡ä»¥å¯ç”¨ä¼˜åŒ–
        os.environ['SERVICE_LOG_DETAILED_EXCEPTIONS'] = 'false'
        os.environ['SERVICE_ENABLE_DETAILED_LOGGING'] = 'false'
        os.environ['ENVIRONMENT'] = 'production'

        # åˆ›å»ºæ ‡å‡†æœåŠ¡å’Œä¼˜åŒ–æœåŠ¡
        standard_service = BaseService()
        optimized_service = PerformanceOptimizedBaseService()

        def test_standard_exception_handling():
            try:
                standard_service._handle_repository_error(
                    ValueError("æµ‹è¯•é”™è¯¯"),
                    "test_operation"
                )
            except Exception:
                pass  # å¿½ç•¥å¼‚å¸¸ï¼Œåªæµ‹è¯•å¤„ç†æ€§èƒ½

        def test_optimized_exception_handling():
            try:
                optimized_service._handle_repository_error(
                    ValueError("æµ‹è¯•é”™è¯¯"),
                    "test_operation"
                )
            except Exception:
                pass  # å¿½ç•¥å¼‚å¸¸ï¼Œåªæµ‹è¯•å¤„ç†æ€§èƒ½

        # æµ‹è¯•æ ‡å‡†å¼‚å¸¸å¤„ç†
        standard_times = []
        for _ in range(self.iterations):
            _, exec_time = self.measure_time(test_standard_exception_handling)
            standard_times.append(exec_time)

        # æµ‹è¯•ä¼˜åŒ–å¼‚å¸¸å¤„ç†
        optimized_times = []
        for _ in range(self.iterations):
            _, exec_time = self.measure_time(test_optimized_exception_handling)
            optimized_times.append(exec_time)

        avg_standard = sum(standard_times) / len(standard_times)
        avg_optimized = sum(optimized_times) / len(optimized_times)
        improvement = ((avg_standard - avg_optimized) / avg_standard) * 100

        print(f"æ ‡å‡†å¼‚å¸¸å¤„ç†å¹³å‡æ—¶é—´: {avg_standard:.3f}ms")
        print(f"ä¼˜åŒ–å¼‚å¸¸å¤„ç†å¹³å‡æ—¶é—´: {avg_optimized:.3f}ms")
        print(f"æ€§èƒ½æå‡: {improvement:.1f}%")

        self.test_results["exception_handling"] = {
            "standard": avg_standard,
            "optimized": avg_optimized,
            "improvement": improvement
        }

        return improvement > 50  # æœŸæœ›è‡³å°‘50%çš„æ€§èƒ½æå‡

    def test_logging_performance_comparison(self):
        """æµ‹è¯•æ—¥å¿—æ€§èƒ½å¯¹æ¯”"""
        print("\n=== æ—¥å¿—æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")

        # åˆ›å»ºæœåŠ¡å®ä¾‹
        standard_service = BaseService()
        optimized_service = PerformanceOptimizedBaseService(enable_detailed_logging=False)

        def test_standard_logging():
            standard_service._log_error("æµ‹è¯•é”™è¯¯æ¶ˆæ¯",
                                     ValueError("æµ‹è¯•å¼‚å¸¸"),
                                     operation="test_operation",
                                     extra_data={"key": "value"})

        def test_optimized_logging():
            optimized_service._log_error("æµ‹è¯•é”™è¯¯æ¶ˆæ¯",
                                       ValueError("æµ‹è¯•å¼‚å¸¸"),
                                       operation="test_operation",
                                       extra_data={"key": "value"})

        # æµ‹è¯•æ ‡å‡†æ—¥å¿—
        standard_times = []
        for _ in range(self.iterations):
            _, exec_time = self.measure_time(test_standard_logging)
            standard_times.append(exec_time)

        # æµ‹è¯•ä¼˜åŒ–æ—¥å¿—
        optimized_times = []
        for _ in range(self.iterations):
            _, exec_time = self.measure_time(test_optimized_logging)
            optimized_times.append(exec_time)

        avg_standard = sum(standard_times) / len(standard_times)
        avg_optimized = sum(optimized_times) / len(optimized_times)
        improvement = ((avg_standard - avg_optimized) / avg_standard) * 100

        print(f"æ ‡å‡†æ—¥å¿—è®°å½•å¹³å‡æ—¶é—´: {avg_standard:.3f}ms")
        print(f"ä¼˜åŒ–æ—¥å¿—è®°å½•å¹³å‡æ—¶é—´: {avg_optimized:.3f}ms")
        print(f"æ€§èƒ½æå‡: {improvement:.1f}%")

        self.test_results["logging"] = {
            "standard": avg_standard,
            "optimized": avg_optimized,
            "improvement": improvement
        }

        return improvement > 30  # æœŸæœ›è‡³å°‘30%çš„æ€§èƒ½æå‡

    def test_service_instantiation_comparison(self):
        """æµ‹è¯•æœåŠ¡å®ä¾‹åŒ–æ€§èƒ½å¯¹æ¯”"""
        print("\n=== æœåŠ¡å®ä¾‹åŒ–æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")

        def create_standard_service():
            return BaseService()

        def create_optimized_service():
            return PerformanceOptimizedBaseService(enable_detailed_logging=False)

        # æµ‹è¯•æ ‡å‡†æœåŠ¡å®ä¾‹åŒ–
        standard_times = []
        for _ in range(50):
            _, exec_time = self.measure_time(create_standard_service)
            standard_times.append(exec_time)

        # æµ‹è¯•ä¼˜åŒ–æœåŠ¡å®ä¾‹åŒ–
        optimized_times = []
        for _ in range(50):
            _, exec_time = self.measure_time(create_optimized_service)
            optimized_times.append(exec_time)

        avg_standard = sum(standard_times) / len(standard_times)
        avg_optimized = sum(optimized_times) / len(optimized_times)
        improvement = ((avg_standard - avg_optimized) / avg_standard) * 100

        print(f"æ ‡å‡†æœåŠ¡å®ä¾‹åŒ–å¹³å‡æ—¶é—´: {avg_standard:.3f}ms")
        print(f"ä¼˜åŒ–æœåŠ¡å®ä¾‹åŒ–å¹³å‡æ—¶é—´: {avg_optimized:.3f}ms")
        print(f"æ€§èƒ½æå‡: {improvement:.1f}%")

        self.test_results["service_instantiation"] = {
            "standard": avg_standard,
            "optimized": avg_optimized,
            "improvement": improvement
        }

        return True  # æœåŠ¡å®ä¾‹åŒ–æ€§èƒ½ä¸è¦æ±‚æå‡ï¼Œåªè¦ä¸æ˜¾è‘—ä¸‹é™å³å¯

    def test_validation_performance_comparison(self):
        """æµ‹è¯•éªŒè¯æ€§èƒ½å¯¹æ¯”"""
        print("\n=== éªŒè¯æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")

        standard_service = BaseService()
        optimized_service = PerformanceOptimizedBaseService()

        test_data = {"name": "test", "value": 123}  # ç¼ºå°‘typeå­—æ®µ
        required_fields = ["name", "value", "type"]

        def test_standard_validation():
            try:
                standard_service.validate_required_fields(test_data, required_fields)
            except Exception:
                pass

        def test_optimized_validation():
            try:
                optimized_service.validate_required_fields_fast(test_data, required_fields)
            except Exception:
                pass

        # æµ‹è¯•æ ‡å‡†éªŒè¯
        standard_times = []
        for _ in range(self.iterations):
            _, exec_time = self.measure_time(test_standard_validation)
            standard_times.append(exec_time)

        # æµ‹è¯•ä¼˜åŒ–éªŒè¯
        optimized_times = []
        for _ in range(self.iterations):
            _, exec_time = self.measure_time(test_optimized_validation)
            optimized_times.append(exec_time)

        avg_standard = sum(standard_times) / len(standard_times)
        avg_optimized = sum(optimized_times) / len(optimized_times)
        improvement = ((avg_standard - avg_optimized) / avg_standard) * 100

        print(f"æ ‡å‡†éªŒè¯å¹³å‡æ—¶é—´: {avg_standard:.3f}Î¼s")
        print(f"ä¼˜åŒ–éªŒè¯å¹³å‡æ—¶é—´: {avg_optimized:.3f}Î¼s")
        print(f"æ€§èƒ½æå‡: {improvement:.1f}%")

        self.test_results["validation"] = {
            "standard": avg_standard,
            "optimized": avg_optimized,
            "improvement": improvement
        }

        return improvement > 20  # æœŸæœ›è‡³å°‘20%çš„æ€§èƒ½æå‡

    def test_optimization_stats(self):
        """æµ‹è¯•ä¼˜åŒ–ç»Ÿè®¡åŠŸèƒ½"""
        print("\n=== ä¼˜åŒ–ç»Ÿè®¡åŠŸèƒ½æµ‹è¯• ===")

        service = PerformanceOptimizedBaseService()

        # æ‰§è¡Œä¸€äº›æ“ä½œä»¥ç”Ÿæˆç»Ÿè®¡æ•°æ®
        try:
            service._handle_repository_error(ValueError("æµ‹è¯•é”™è¯¯"), "test_op")
        except Exception:
            pass

        try:
            service.validate_required_fields_fast({"name": "test"}, ["name", "value"])
        except Exception:
            pass

        # è·å–ä¼˜åŒ–ç»Ÿè®¡
        stats = service.get_optimization_stats()

        print(f"æœåŠ¡åç§°: {stats['service_name']}")
        print(f"æ€§èƒ½ä¼˜åŒ–å¯ç”¨: {stats['performance_optimization']}")
        print(f"è¯¦ç»†æ—¥å¿—: {stats['detailed_logging']}")
        print(f"æ“ä½œè®¡æ•°: {stats['operation_count']}")
        print(f"å¼‚å¸¸å¤„ç†ç»Ÿè®¡: {stats['exception_handling']}")

        return True

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰ä¼˜åŒ–æ•ˆæœéªŒè¯æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ€§èƒ½ä¼˜åŒ–æ•ˆæœéªŒè¯æµ‹è¯•")
        print("="*60)

        test_results = {
            "exception_handling": self.test_exception_handling_overhead_comparison(),
            "logging": self.test_logging_performance_comparison(),
            "service_instantiation": self.test_service_instantiation_comparison(),
            "validation": self.test_validation_performance_comparison(),
            "optimization_stats": self.test_optimization_stats()
        }

        self._print_final_results(test_results)

        return test_results

    def _print_final_results(self, test_results: Dict[str, bool]):
        """æ‰“å°æœ€ç»ˆæµ‹è¯•ç»“æœ"""
        print("\n" + "="*60)
        print("ä¼˜åŒ–æ•ˆæœéªŒè¯ç»“æœ")
        print("="*60)

        passed_tests = sum(test_results.values())
        total_tests = len(test_results)

        for test_name, passed in test_results.items():
            status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
            improvement = self.test_results.get(test_name, {}).get("improvement", 0)
            print(f"{test_name}: {status} (æ€§èƒ½æå‡: {improvement:.1f}%)")

        print(f"\nğŸ“Š æ€»ä½“ç»“æœ: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡")

        if passed_tests == total_tests:
            print("ğŸ‰ æ‰€æœ‰ä¼˜åŒ–æµ‹è¯•éƒ½é€šè¿‡äº†ï¼æ€§èƒ½ä¼˜åŒ–æ•ˆæœæ˜¾è‘—ã€‚")
        elif passed_tests >= total_tests * 0.8:
            print("ğŸ‘ å¤§éƒ¨åˆ†ä¼˜åŒ–æµ‹è¯•é€šè¿‡ï¼Œæ€§èƒ½ä¼˜åŒ–åŸºæœ¬æˆåŠŸã€‚")
        else:
            print("âš ï¸ éƒ¨åˆ†ä¼˜åŒ–æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæ•´ä¼˜åŒ–ç­–ç•¥ã€‚")

        # è®¡ç®—æ€»ä½“æ€§èƒ½æå‡
        improvements = [result.get("improvement", 0) for result in self.test_results.values()]
        if improvements:
            avg_improvement = sum(improvements) / len(improvements)
            print(f"ğŸ“ˆ å¹³å‡æ€§èƒ½æå‡: {avg_improvement:.1f}%")


def main():
    """ä¸»å‡½æ•°"""
    test = OptimizationResultTest()
    test.run_all_tests()
    print("\nâœ… æ€§èƒ½ä¼˜åŒ–æ•ˆæœéªŒè¯æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()