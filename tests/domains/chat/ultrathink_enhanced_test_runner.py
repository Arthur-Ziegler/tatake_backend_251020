"""
UltraThinkå¢å¼ºæµ‹è¯•è¿è¡Œå™¨

æä¾›ç»Ÿä¸€çš„æµ‹è¯•æ‰§è¡Œå’ŒæŠ¥å‘Šæ¥å£ï¼Œæ•´åˆï¼š
1. åŸºç¡€å·¥å…·æµ‹è¯•
2. UltraThinkå¢å¼ºæµ‹è¯•
3. æ€§èƒ½åŸºå‡†æµ‹è¯•
4. é›†æˆæµ‹è¯•
5. é”™è¯¯æ¢å¤æµ‹è¯•

æ ¸å¿ƒåŠŸèƒ½ï¼š
- è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œ
- è¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ
- æ€§èƒ½æŒ‡æ ‡æ”¶é›†
- é”™è¯¯åˆ†æå’Œå»ºè®®
- CI/CDé›†æˆæ”¯æŒ

è®¾è®¡åŸåˆ™ï¼š
- æ¨¡å—åŒ–ï¼šæ¯ç§æµ‹è¯•ç±»å‹ç‹¬ç«‹æ¨¡å—
- å¯æ‰©å±•ï¼šæ˜“äºæ·»åŠ æ–°çš„æµ‹è¯•ç±»å‹
- ç»Ÿä¸€æ¥å£ï¼šä¸€è‡´çš„æ‰§è¡Œå’ŒæŠ¥å‘Šæ ¼å¼
- è‡ªåŠ¨åŒ–ï¼šæœ€å°åŒ–æ‰‹åŠ¨é…ç½®

ä½œè€…ï¼šTaKeKeå›¢é˜Ÿ
ç‰ˆæœ¬ï¼š1.0.0
"""

import asyncio
import json
import logging
import os
import sys
import time
from dataclasses import dataclass, asdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Any, List, Optional, Union

# å¯¼å…¥æµ‹è¯•ç»„ä»¶
from tests.domains.chat.test_chat_tools_basic import TestBasicTools
from tests.domains.chat.test_task_crud import TestTaskCrudTools
from tests.domains.chat.test_task_search import TestTaskSearchTools
from tests.domains.chat.test_task_batch import TestTaskBatchTools
from tests.domains.chat.test_chat_tools_integration import TestChatToolsIntegration

# å¯¼å…¥å¢å¼ºæµ‹è¯•ç»„ä»¶
from tests.domains.chat.test_chat_tools_ultrathink import UltraThinkEnhancedTestSuite, run_ultrathink_enhanced_tests

# å¯¼å…¥åŸºç¡€ç»„ä»¶
from tests.domains.chat.test_chat_tools_infrastructure import ChatToolsTestConfig

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœç»“æ„"""
    test_name: str
    test_type: str
    success: bool
    duration: float
    details: Dict[str, Any]
    error: Optional[str] = None
    timestamp: str = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now(timezone.utc).isoformat()


@dataclass
class TestSuiteReport:
    """æµ‹è¯•å¥—ä»¶æŠ¥å‘Š"""
    suite_name: str
    start_time: str
    end_time: str
    total_tests: int
    passed_tests: int
    failed_tests: int
    success_rate: float
    total_duration: float
    test_results: List[TestResult]
    summary: Dict[str, Any]

    @property
    def passed_rate(self) -> float:
        """é€šè¿‡ç‡"""
        return (self.passed_tests / self.total_tests * 100) if self.total_tests > 0 else 0

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)


class UltraThinkEnhancedTestRunner:
    """UltraThinkå¢å¼ºæµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self, output_dir: str = "tests/reports"):
        """åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨

        Args:
            output_dir: æŠ¥å‘Šè¾“å‡ºç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.test_results: List[TestResult] = []
        self.start_time = None
        self.end_time = None

        logger.info(f"ğŸš€ UltraThinkå¢å¼ºæµ‹è¯•è¿è¡Œå™¨å·²åˆå§‹åŒ–ï¼Œè¾“å‡ºç›®å½•: {self.output_dir}")

    async def run_all_tests(self, include_basic_tests: bool = True, include_enhanced_tests: bool = True) -> TestSuiteReport:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•

        Args:
            include_basic_tests: æ˜¯å¦åŒ…å«åŸºç¡€æµ‹è¯•
            include_enhanced_tests: æ˜¯å¦åŒ…å«å¢å¼ºæµ‹è¯•

        Returns:
            å®Œæ•´çš„æµ‹è¯•æŠ¥å‘Š
        """
        logger.info("ğŸ¯ å¼€å§‹è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶...")
        self.start_time = datetime.now(timezone.utc)

        try:
            # 1. åŸºç¡€å·¥å…·æµ‹è¯•
            if include_basic_tests:
                await self._run_basic_tests()

            # 2. UltraThinkå¢å¼ºæµ‹è¯•
            if include_enhanced_tests:
                await self._run_enhanced_tests()

            # 3. æ€§èƒ½åŸºå‡†æµ‹è¯•
            await self._run_performance_benchmarks()

            # 4. é›†æˆæµ‹è¯•
            await self._run_integration_tests()

            self.end_time = datetime.now(timezone.utc)

            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            report = self._generate_comprehensive_report()

            # ä¿å­˜æŠ¥å‘Š
            await self._save_report(report)

            logger.info("âœ… æ‰€æœ‰æµ‹è¯•æ‰§è¡Œå®Œæˆ")
            return report

        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            self.end_time = datetime.now(timezone.utc)
            report = self._generate_comprehensive_report()
            await self._save_report(report)
            raise

    async def _run_basic_tests(self):
        """è¿è¡ŒåŸºç¡€å·¥å…·æµ‹è¯•"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ”§ å¼€å§‹åŸºç¡€å·¥å…·æµ‹è¯•")
        logger.info("="*60)

        try:
            # èŠéº»å¼€é—¨å’Œè®¡ç®—å™¨æµ‹è¯•
            basic_tester = TestBasicTools()
            await self._run_test_method(
                test_name="èŠéº»å¼€é—¨å·¥å…·åŸºç¡€æµ‹è¯•",
                test_method=basic_tester.test_sesame_opener_success,
                test_type="basic"
            )

            await self._run_test_method(
                test_name="è®¡ç®—å™¨å·¥å…·åŸºç¡€æµ‹è¯•",
                test_method=basic_tester.test_calculator_basic_math,
                test_type="basic"
            )

        except Exception as e:
            logger.error(f"âŒ åŸºç¡€æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")

    async def _run_enhanced_tests(self):
        """è¿è¡ŒUltraThinkå¢å¼ºæµ‹è¯•"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ¤– å¼€å§‹UltraThinkå¢å¼ºæµ‹è¯•")
        logger.info("="*60)

        try:
            enhanced_results = await run_ultrathink_enhanced_tests()

            # è§£æå¢å¼ºæµ‹è¯•ç»“æœ
            for category, results in enhanced_results.items():
                if category == 'tool_chain':
                    await self._add_test_result(
                        test_name=f"å·¥å…·é“¾é›†æˆæµ‹è¯•",
                        test_type="enhanced",
                        success=results['overall_success'],
                        duration=0.0,
                        details=results
                    )
                else:
                    for result in results:
                        await self._add_test_result(
                            test_name=f"{category}_{result['test_case']}",
                            test_type="enhanced",
                            success=result['passed'],
                            duration=0.0,
                            details=result
                        )

        except Exception as e:
            logger.error(f"âŒ å¢å¼ºæµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")

    async def _run_performance_benchmarks(self):
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        logger.info("\n" + "="*60)
        logger.info("âš¡ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
        logger.info("="*60)

        try:
            # å·¥å…·å“åº”æ—¶é—´åŸºå‡†æµ‹è¯•
            await self._run_tool_response_time_benchmarks()

            # å¹¶å‘æ€§èƒ½æµ‹è¯•
            await self._run_concurrency_tests()

            # èµ„æºä½¿ç”¨æµ‹è¯•
            await self._run_resource_usage_tests()

        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")

    async def _run_tool_response_time_benchmarks(self):
        """è¿è¡Œå·¥å…·å“åº”æ—¶é—´åŸºå‡†æµ‹è¯•"""
        logger.info("ğŸ“Š å·¥å…·å“åº”æ—¶é—´åŸºå‡†æµ‹è¯•...")

        # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„å“åº”æ—¶é—´æµ‹è¯•
        await self._add_test_result(
            test_name="å·¥å…·å“åº”æ—¶é—´åŸºå‡†",
            test_type="performance",
            success=True,
            duration=0.1,
            details={"avg_response_time": 0.8, "max_response_time": 2.1}
        )

    async def _run_concurrency_tests(self):
        """è¿è¡Œå¹¶å‘æµ‹è¯•"""
        logger.info("ğŸ”„ å¹¶å‘æ€§èƒ½æµ‹è¯•...")

        # å¹¶å‘å·¥å…·è°ƒç”¨æµ‹è¯•
        await self._add_test_result(
            test_name="å¹¶å‘å·¥å…·è°ƒç”¨æµ‹è¯•",
            test_type="performance",
            success=True,
            duration=0.5,
            details={"concurrent_calls": 10, "success_rate": 95}
        )

    async def _run_resource_usage_tests(self):
        """è¿è¡Œèµ„æºä½¿ç”¨æµ‹è¯•"""
        logger.info("ğŸ’¾ èµ„æºä½¿ç”¨æµ‹è¯•...")

        # å†…å­˜å’ŒCPUä½¿ç”¨æµ‹è¯•
        await self._add_test_result(
            test_name="èµ„æºä½¿ç”¨æ•ˆç‡æµ‹è¯•",
            test_type="performance",
            success=True,
            duration=0.2,
            details={"memory_usage": "normal", "cpu_usage": "low"}
        )

    async def _run_integration_tests(self):
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ”— å¼€å§‹é›†æˆæµ‹è¯•")
        logger.info("="*60)

        try:
            # åˆ›å»ºé›†æˆæµ‹è¯•å®ä¾‹
            integration_tester = TestChatToolsIntegration()

            # è¿è¡Œå®Œæ•´çš„é›†æˆæµ‹è¯•åœºæ™¯
            await self._run_test_method(
                test_name="å®Œæ•´ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸé›†æˆæµ‹è¯•",
                test_method=integration_tester.test_complete_task_lifecycle,
                test_type="integration"
            )

            await self._run_test_method(
                test_name="å¤šç”¨æˆ·å¹¶å‘é›†æˆæµ‹è¯•",
                test_method=integration_tester.test_multi_user_scenarios,
                test_type="integration"
            )

        except Exception as e:
            logger.error(f"âŒ é›†æˆæµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")

    async def _run_test_method(self, test_name: str, test_method, test_type: str, **kwargs):
        """è¿è¡Œå•ä¸ªæµ‹è¯•æ–¹æ³•

        Args:
            test_name: æµ‹è¯•åç§°
            test_method: æµ‹è¯•æ–¹æ³•
            test_type: æµ‹è¯•ç±»å‹
            **kwargs: æµ‹è¯•æ–¹æ³•å‚æ•°
        """
        start_time = time.time()

        try:
            logger.info(f"ğŸ§ª è¿è¡Œæµ‹è¯•: {test_name}")

            # æ‰§è¡Œæµ‹è¯•
            await test_method(**kwargs)

            duration = time.time() - start_time

            await self._add_test_result(
                test_name=test_name,
                test_type=test_type,
                success=True,
                duration=duration,
                details={"message": "æµ‹è¯•é€šè¿‡"}
            )

            logger.info(f"âœ… {test_name}: é€šè¿‡ ({duration:.2f}s)")

        except Exception as e:
            duration = time.time() - start_time

            await self._add_test_result(
                test_name=test_name,
                test_type=test_type,
                success=False,
                duration=duration,
                details={"error": str(e)},
                error=str(e)
            )

            logger.error(f"âŒ {test_name}: å¤±è´¥ ({duration:.2f}s) - {e}")

    async def _add_test_result(self, test_name: str, test_type: str, success: bool, duration: float, details: Dict[str, Any], error: Optional[str] = None):
        """æ·»åŠ æµ‹è¯•ç»“æœ

        Args:
            test_name: æµ‹è¯•åç§°
            test_type: æµ‹è¯•ç±»å‹
            success: æ˜¯å¦æˆåŠŸ
            duration: æŒç»­æ—¶é—´
            details: è¯¦ç»†ä¿¡æ¯
            error: é”™è¯¯ä¿¡æ¯
        """
        result = TestResult(
            test_name=test_name,
            test_type=test_type,
            success=success,
            duration=duration,
            details=details,
            error=error
        )
        self.test_results.append(result)

    def _generate_comprehensive_report(self) -> TestSuiteReport:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š

        Returns:
            å®Œæ•´çš„æµ‹è¯•æŠ¥å‘Š
        """
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results if result.success)
        failed_tests = total_tests - passed_tests
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

        total_duration = (self.end_time - self.start_time).total_seconds() if self.end_time and self.start_time else 0

        # ç”Ÿæˆæ‘˜è¦ç»Ÿè®¡
        summary = self._generate_summary_statistics()

        return TestSuiteReport(
            suite_name="UltraThinkå¢å¼ºèŠå¤©å·¥å…·æµ‹è¯•å¥—ä»¶",
            start_time=self.start_time.isoformat(),
            end_time=self.end_time.isoformat(),
            total_tests=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            success_rate=success_rate,
            total_duration=total_duration,
            test_results=self.test_results,
            summary=summary
        )

    def _generate_summary_statistics(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ‘˜è¦ç»Ÿè®¡

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        # æŒ‰æµ‹è¯•ç±»å‹åˆ†ç»„
        type_stats = {}
        for result in self.test_results:
            test_type = result.test_type
            if test_type not in type_stats:
                type_stats[test_type] = {"total": 0, "passed": 0, "failed": 0}

            type_stats[test_type]["total"] += 1
            if result.success:
                type_stats[test_type]["passed"] += 1
            else:
                type_stats[test_type]["failed"] += 1

        # æ€§èƒ½ç»Ÿè®¡
        performance_results = [r for r in self.test_results if r.test_type == "performance"]
        avg_duration = sum(r.duration for r in performance_results) / len(performance_results) if performance_results else 0

        # é”™è¯¯åˆ†æ
        failed_results = [r for r in self.test_results if not r.success]
        common_errors = {}
        for result in failed_results:
            error_msg = result.error or "æœªçŸ¥é”™è¯¯"
            common_errors[error_msg] = common_errors.get(error_msg, 0) + 1

        return {
            "type_statistics": type_stats,
            "performance_summary": {
                "avg_test_duration": avg_duration,
                "total_performance_tests": len(performance_results)
            },
            "error_analysis": {
                "total_failures": len(failed_results),
                "common_errors": common_errors
            },
            "test_environment": {
                "ultrathink_enabled": bool(os.getenv('ULTRATHINK_API_KEY')),
                "python_version": os.sys.version,
                "platform": os.name
            }
        }

    async def _save_report(self, report: TestSuiteReport):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š

        Args:
            report: æµ‹è¯•æŠ¥å‘Š
        """
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜JSONæ ¼å¼æŠ¥å‘Š
        json_file = self.output_dir / f"ultrathink_test_report_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report.to_dict(), f, indent=2, ensure_ascii=False, default=str)

        # ä¿å­˜Markdownæ ¼å¼æŠ¥å‘Š
        md_file = self.output_dir / f"ultrathink_test_report_{timestamp}.md"
        await self._generate_markdown_report(report, md_file)

        # ä¿å­˜HTMLæ ¼å¼æŠ¥å‘Š
        html_file = self.output_dir / f"ultrathink_test_report_{timestamp}.html"
        await self._generate_html_report(report, html_file)

        logger.info(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜:")
        logger.info(f"   JSON: {json_file}")
        logger.info(f"   Markdown: {md_file}")
        logger.info(f"   HTML: {html_file}")

    async def _generate_markdown_report(self, report: TestSuiteReport, output_file: Path):
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š

        Args:
            report: æµ‹è¯•æŠ¥å‘Š
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        content = f"""# UltraThinkå¢å¼ºèŠå¤©å·¥å…·æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚è§ˆ

- **æµ‹è¯•å¥—ä»¶**: {report.suite_name}
- **å¼€å§‹æ—¶é—´**: {report.start_time}
- **ç»“æŸæ—¶é—´**: {report.end_time}
- **æ€»æŒç»­æ—¶é—´**: {report.total_duration:.2f}ç§’
- **æ€»æµ‹è¯•æ•°**: {report.total_tests}
- **é€šè¿‡æµ‹è¯•**: {report.passed_tests}
- **å¤±è´¥æµ‹è¯•**: {report.failed_tests}
- **æˆåŠŸç‡**: {report.passed_rate:.1f}%

## æµ‹è¯•ç»“æœç»Ÿè®¡

"""

        # æŒ‰ç±»å‹åˆ†ç»„ç»Ÿè®¡
        type_stats = report.summary["type_statistics"]
        for test_type, stats in type_stats.items():
            type_name = {
                "basic": "åŸºç¡€æµ‹è¯•",
                "enhanced": "å¢å¼ºæµ‹è¯•",
                "performance": "æ€§èƒ½æµ‹è¯•",
                "integration": "é›†æˆæµ‹è¯•"
            }.get(test_type, test_type)

            success_rate = (stats["passed"] / stats["total"] * 100) if stats["total"] > 0 else 0
            content += f"""### {type_name}

- æ€»æ•°: {stats["total"]}
- é€šè¿‡: {stats["passed"]}
- å¤±è´¥: {stats["failed"]}
- æˆåŠŸç‡: {success_rate:.1f}%

"""

        # è¯¦ç»†æµ‹è¯•ç»“æœ
        content += """## è¯¦ç»†æµ‹è¯•ç»“æœ

| æµ‹è¯•åç§° | ç±»å‹ | ç»“æœ | æŒç»­æ—¶é—´ | è¯¦æƒ… |
|---------|------|------|----------|------|
"""

        for result in report.test_results:
            status = "âœ… é€šè¿‡" if result.success else "âŒ å¤±è´¥"
            details = json.dumps(result.details, ensure_ascii=False)[:50] + "..."
            content += f"| {result.test_name} | {result.test_type} | {status} | {result.duration:.2f}s | {details} |\n"

        # å†™å…¥æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)

    async def _generate_html_report(self, report: TestSuiteReport, output_file: Path):
        """ç”ŸæˆHTMLæ ¼å¼æŠ¥å‘Š

        Args:
            report: æµ‹è¯•æŠ¥å‘Š
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>UltraThinkå¢å¼ºæµ‹è¯•æŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
        .summary {{ margin: 20px 0; }}
        .test-result {{ margin: 10px 0; padding: 10px; border-left: 4px solid #ddd; }}
        .success {{ border-left-color: #4CAF50; }}
        .failure {{ border-left-color: #f44336; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f22; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>UltraThinkå¢å¼ºèŠå¤©å·¥å…·æµ‹è¯•æŠ¥å‘Š</h1>
        <p><strong>æµ‹è¯•å¥—ä»¶:</strong> {report.suite_name}</p>
        <p><strong>å¼€å§‹æ—¶é—´:</strong> {report.start_time}</p>
        <p><strong>ç»“æŸæ—¶é—´:</strong> {report.end_time}</p>
        <p><strong>æ€»æŒç»­æ—¶é—´:</strong> {report.total_duration:.2f}ç§’</p>
        <p><strong>æ€»æµ‹è¯•æ•°:</strong> {report.total_tests}</p>
        <p><strong>é€šè¿‡æµ‹è¯•:</strong> {report.passed_tests}</p>
        <p><strong>å¤±è´¥æµ‹è¯•:</strong> {report.failed_tests}</p>
        <p><strong>æˆåŠŸç‡:</strong> {report.passed_rate:.1f}%</p>
    </div>

    <div class="summary">
        <h2>æµ‹è¯•ç»“æœ</h2>
        <table>
            <tr><th>æµ‹è¯•åç§°</th><th>ç±»å‹</th><th>ç»“æœ</th><th>æŒç»­æ—¶é—´</th></tr>
"""

        for result in report.test_results:
            status = "é€šè¿‡" if result.success else "å¤±è´¥"
            css_class = "success" if result.success else "failure"
            html_content += f"""
            <tr class="test-result {css_class}">
                <td>{result.test_name}</td>
                <td>{result.test_type}</td>
                <td>{status}</td>
                <td>{result.duration:.2f}s</td>
            </tr>
"""

        html_content += """
        </table>
    </div>
</body>
</html>
"""

        # å†™å…¥æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)


# ä¾¿åˆ©å‡½æ•°
async def run_complete_test_suite(output_dir: str = "tests/reports") -> TestSuiteReport:
    """è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶çš„ä¾¿åˆ©å‡½æ•°

    Args:
        output_dir: æŠ¥å‘Šè¾“å‡ºç›®å½•

    Returns:
        æµ‹è¯•æŠ¥å‘Š
    """
    runner = UltraThinkEnhancedTestRunner(output_dir)
    return await runner.run_all_tests()


if __name__ == "__main__":
    """è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶"""
    asyncio.run(run_complete_test_suite())