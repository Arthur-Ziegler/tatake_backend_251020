# æœåŠ¡å±‚æ—¥å¿—ç³»ç»Ÿè¯¦è§£

## æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»æœåŠ¡å±‚çš„ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿï¼ŒåŒ…æ‹¬é…ç½®æ–¹æ³•ã€ä½¿ç”¨æŒ‡å—ã€æ€§èƒ½ç›‘æ§å’Œæœ€ä½³å®è·µã€‚

## ğŸ—ï¸ æ—¥å¿—ç³»ç»Ÿæ¶æ„

### æ ¸å¿ƒç»„ä»¶

```
ServiceLogger (æœåŠ¡æ—¥å¿—å™¨)
â”œâ”€â”€ ServiceLoggerConfig (é…ç½®ç®¡ç†)
â”œâ”€â”€ StructuredFormatter (ç»“æ„åŒ–æ ¼å¼åŒ–å™¨)
â”œâ”€â”€ SimpleFormatter (ç®€å•æ ¼å¼åŒ–å™¨)
â””â”€â”€ è£…é¥°å™¨ (Decorators)
    â”œâ”€â”€ @operation_logger (æ“ä½œæ—¥å¿—è£…é¥°å™¨)
    â””â”€â”€ @performance_logger (æ€§èƒ½ç›‘æ§è£…é¥°å™¨)
```

### æ—¥å¿—æµç¨‹

```
Service Method â†’ ServiceLogger â†’ Formatter â†’ Output (Console/File)
                                    â†“
                            Structured JSON Format
```

## ğŸ”§ é…ç½®ç³»ç»Ÿ

### ç¯å¢ƒå˜é‡é…ç½®

| å˜é‡å | é»˜è®¤å€¼ | è¯´æ˜ | å¯é€‰å€¼ |
|--------|--------|------|--------|
| `SERVICE_LOG_LEVEL` | INFO | æ—¥å¿—çº§åˆ« | DEBUG/INFO/WARNING/ERROR/CRITICAL |
| `SERVICE_LOG_FORMAT` | structured | æ—¥å¿—æ ¼å¼ | structured/simple |
| `SERVICE_LOG_CONSOLE` | true | æ§åˆ¶å°è¾“å‡º | true/false |
| `SERVICE_LOG_FILE` | true | æ–‡ä»¶è¾“å‡º | true/false |
| `SERVICE_LOG_FILE_PATH` | logs/services.log | æ—¥å¿—æ–‡ä»¶è·¯å¾„ | ä»»æ„æœ‰æ•ˆè·¯å¾„ |
| `SERVICE_LOG_MAX_SIZE` | 10 | æ—¥å¿—æ–‡ä»¶æœ€å¤§å¤§å°(MB) | 1-100 |
| `SERVICE_LOG_BACKUP_COUNT` | 5 | æ—¥å¿—å¤‡ä»½æ•°é‡ | 1-20 |
| `SERVICE_LOG_PERFORMANCE` | false | æ€§èƒ½ç›‘æ§å¼€å…³ | true/false |

### é…ç½®ç¤ºä¾‹

#### å¼€å‘ç¯å¢ƒé…ç½®

```bash
# .env.dev
SERVICE_LOG_LEVEL=DEBUG
SERVICE_LOG_FORMAT=simple
SERVICE_LOG_CONSOLE=true
SERVICE_LOG_FILE=false
SERVICE_LOG_PERFORMANCE=true
```

#### ç”Ÿäº§ç¯å¢ƒé…ç½®

```bash
# .env.prod
SERVICE_LOG_LEVEL=INFO
SERVICE_LOG_FORMAT=structured
SERVICE_LOG_CONSOLE=false
SERVICE_LOG_FILE=true
SERVICE_LOG_FILE_PATH=/var/log/tatake/services.log
SERVICE_LOG_MAX_SIZE=50
SERVICE_LOG_BACKUP_COUNT=10
SERVICE_LOG_PERFORMANCE=true
```

#### æµ‹è¯•ç¯å¢ƒé…ç½®

```bash
# .env.test
SERVICE_LOG_LEVEL=ERROR
SERVICE_LOG_FORMAT=simple
SERVICE_LOG_CONSOLE=false
SERVICE_LOG_FILE=false
SERVICE_LOG_PERFORMANCE=false
```

## ğŸ“ æ—¥å¿—æ ¼å¼

### ç»“æ„åŒ–JSONæ ¼å¼ (Structured)

```json
{
  "timestamp": "2025-10-20T10:30:45.123456Z",
  "level": "INFO",
  "logger": "services.TaskService",
  "message": "ä»»åŠ¡åˆ›å»ºæˆåŠŸ",
  "module": "task_service",
  "function": "create_task",
  "line": 245,
  "service": "TaskService",
  "operation": "create_task",
  "user_id": "user_123",
  "session_id": "sess_456",
  "request_id": "req_789",
  "duration_ms": 125.5,
  "extra_data": {
    "task_id": "task_abc",
    "priority": "high"
  }
}
```

### ç®€å•æ ¼å¼ (Simple)

```
2025-10-20 10:30:45 | INFO     | services.TaskService | create_task:245 | ä»»åŠ¡åˆ›å»ºæˆåŠŸ
```

### å­—æ®µè¯´æ˜

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `timestamp` | string | ISO 8601æ ¼å¼çš„æ—¶é—´æˆ³ |
| `level` | string | æ—¥å¿—çº§åˆ« (DEBUG/INFO/WARNING/ERROR/CRITICAL) |
| `logger` | string | æ—¥å¿—å™¨åç§°ï¼Œæ ¼å¼ä¸º `services.{ServiceName}` |
| `message` | string | æ—¥å¿—æ¶ˆæ¯ |
| `module` | string | Pythonæ¨¡å—å |
| `function` | string | å‡½æ•°å |
| `line` | number | ä»£ç è¡Œå· |
| `service` | string | æœåŠ¡ç±»å |
| `operation` | string | æ“ä½œåç§°ï¼ˆè£…é¥°å™¨è‡ªåŠ¨æ·»åŠ ï¼‰ |
| `user_id` | string | ç”¨æˆ·IDï¼ˆå¯é€‰ï¼‰ |
| `session_id` | string | ä¼šè¯IDï¼ˆå¯é€‰ï¼‰ |
| `request_id` | string | è¯·æ±‚IDï¼ˆå¯é€‰ï¼‰ |
| `duration_ms` | number | æ“ä½œè€—æ—¶ï¼ˆæ¯«ç§’ï¼Œæ€§èƒ½æ—¥å¿—ï¼‰ |
| `extra_data` | object | é¢å¤–çš„ä¸Šä¸‹æ–‡æ•°æ® |

## ğŸ¯ ä½¿ç”¨æ–¹æ³•

### 1. åŸºæœ¬æ—¥å¿—è®°å½•

```python
class TaskService(BaseService):
    def create_task(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        # åŸºæœ¬æ—¥å¿—
        self._log_info("å¼€å§‹åˆ›å»ºä»»åŠ¡", extra_data={"title": task_data.get("title")})

        try:
            # ä¸šåŠ¡é€»è¾‘
            task = self._create_task_in_database(task_data)

            # æˆåŠŸæ—¥å¿—
            self._log_info("ä»»åŠ¡åˆ›å»ºæˆåŠŸ", extra_data={"task_id": task.id})
            return task

        except Exception as e:
            # é”™è¯¯æ—¥å¿—
            self._log_error("ä»»åŠ¡åˆ›å»ºå¤±è´¥", error=e, extra_data={"task_data": task_data})
            raise
```

### 2. ä¸åŒçº§åˆ«çš„æ—¥å¿—

```python
def demonstrate_log_levels(self):
    # DEBUGçº§åˆ«ï¼šè¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
    self._log_debug("è°ƒè¯•ä¿¡æ¯", variable_value=42, data_structure={"key": "value"})

    # INFOçº§åˆ«ï¼šä¸€èˆ¬ä¿¡æ¯
    self._log_info("æ“ä½œå®Œæˆ", operation="user_login", user_id="123")

    # WARNINGçº§åˆ«ï¼šè­¦å‘Šä¿¡æ¯
    self._log_warning("æ€§èƒ½è­¦å‘Š", operation="slow_query", duration_ms=5000)

    # ERRORçº§åˆ«ï¼šé”™è¯¯ä¿¡æ¯
    self._log_error("æ“ä½œå¤±è´¥", error=exception, operation="database_update")

    # CRITICALçº§åˆ«ï¼šä¸¥é‡é”™è¯¯
    self._log_critical("ç³»ç»Ÿæ•…éšœ", component="database", error_code="DB_CONN_FAILED")
```

### 3. æ“ä½œæ—¥å¿—è£…é¥°å™¨

```python
from src.services.logging_config import operation_logger

class TaskService(BaseService):
    @operation_logger("åˆ›å»ºä»»åŠ¡", log_args=True, log_result=True)
    def create_task(self, user_id: str, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ›å»ºä»»åŠ¡
        è£…é¥°å™¨ä¼šè‡ªåŠ¨è®°å½•ï¼š
        - æ“ä½œå¼€å§‹
        - æ“ä½œå‚æ•° (log_args=True)
        - æ“ä½œç»“æœ (log_result=True)
        - æ“ä½œæˆåŠŸ/å¤±è´¥
        - æ‰§è¡Œæ—¶é—´
        """
        # éªŒè¯å‚æ•°
        self.validate_required_fields(task_data, ["title", "description"])

        # åˆ›å»ºä»»åŠ¡
        task = self.get_task_repository().create({
            "user_id": user_id,
            **task_data,
            "created_at": datetime.now()
        })

        return self._task_to_dict(task)

    @operation_logger("åˆ é™¤ä»»åŠ¡", log_args=False, log_result=False)
    def delete_task(self, task_id: str, user_id: str) -> None:
        """
        åˆ é™¤ä»»åŠ¡
        ä¸è®°å½•å‚æ•°å’Œç»“æœï¼Œä½†è®°å½•æ“ä½œçŠ¶æ€
        """
        # æ£€æŸ¥æƒé™
        task = self._check_task_ownership(task_id, user_id)

        # åˆ é™¤ä»»åŠ¡
        self.get_task_repository().delete(task_id)
```

### 4. æ€§èƒ½ç›‘æ§è£…é¥°å™¨

```python
from src.services.logging_config import performance_logger

class StatisticsService(BaseService):
    @performance_logger("ç”Ÿæˆæœˆåº¦æŠ¥å‘Š")
    def generate_monthly_report(self, user_id: str, month: str) -> Dict[str, Any]:
        """
        ç”Ÿæˆæœˆåº¦æŠ¥å‘Š
        è£…é¥°å™¨ä¼šè‡ªåŠ¨è®°å½•æ‰§è¡Œæ—¶é—´
        """
        # å¤æ‚çš„æ•°æ®å¤„ç†é€»è¾‘
        tasks = self.get_task_repository().find_by_user_and_month(user_id, month)
        focus_sessions = self.get_focus_repository().find_by_user_and_month(user_id, month)

        report = self._calculate_monthly_statistics(tasks, focus_sessions)
        return report

    @performance_logger("æ‰¹é‡æ•°æ®å¤„ç†")
    def batch_process_data(self, data_list: List[Dict]) -> List[Dict]:
        """
        æ‰¹é‡å¤„ç†æ•°æ®
        é€‚åˆç›‘æ§è€—æ—¶è¾ƒé•¿çš„æ‰¹é‡æ“ä½œ
        """
        results = []
        for data in data_list:
            processed = self._process_single_item(data)
            results.append(processed)

        return results
```

### 5. ä¸Šä¸‹æ–‡æ—¥å¿—

```python
class UserService(BaseService):
    def update_user_profile(self, user_id: str, profile_data: Dict) -> Dict[str, Any]:
        # å¼€å§‹æ“ä½œï¼Œè®°å½•ä¸Šä¸‹æ–‡
        self._log_operation_start("update_user_profile",
                                 user_id=user_id,
                                 update_fields=list(profile_data.keys()))

        try:
            # éªŒè¯ç”¨æˆ·å­˜åœ¨
            user = self._check_resource_exists(
                self.get_user_repository(),
                user_id,
                "ç”¨æˆ·"
            )

            # æ›´æ–°ç”¨æˆ·ä¿¡æ¯
            updated_user = self.get_user_repository().update(user_id, profile_data)

            # è®°å½•æˆåŠŸï¼ŒåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯
            self._log_operation_success("update_user_profile",
                                        user_id=user_id,
                                        updated_fields=list(profile_data.keys()),
                                        previous_version=user.version)

            return self._user_to_dict(updated_user)

        except Exception as e:
            # è®°å½•å¤±è´¥ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯
            self._log_operation_error("update_user_profile", e,
                                     user_id=user_id,
                                     update_fields=list(profile_data.keys()))
            raise
```

## ğŸ” é«˜çº§åŠŸèƒ½

### 1. è‡ªå®šä¹‰æ—¥å¿—ä¸Šä¸‹æ–‡

```python
class CustomService(BaseService):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # è®¾ç½®è‡ªå®šä¹‰æ—¥å¿—ä¸Šä¸‹æ–‡
        self._log_context = {
            "service_version": "2.1.0",
            "environment": os.getenv("ENVIRONMENT", "development"),
            "instance_id": os.getenv("INSTANCE_ID", "local")
        }

    def _log_with_context(self, level: str, message: str, **kwargs):
        """å¸¦è‡ªå®šä¹‰ä¸Šä¸‹æ–‡çš„æ—¥å¿—è®°å½•"""
        all_kwargs = {**self._log_context, **kwargs}

        if level == "DEBUG":
            self._log_debug(message, **all_kwargs)
        elif level == "INFO":
            self._log_info(message, **all_kwargs)
        elif level == "WARNING":
            self._log_warning(message, **all_kwargs)
        elif level == "ERROR":
            self._log_error(message, **all_kwargs)
        elif level == "CRITICAL":
            self._log_critical(message, **all_kwargs)

    def process_data(self, data: Dict):
        self._log_with_context("INFO", "å¼€å§‹å¤„ç†æ•°æ®",
                              data_type=data.get("type"),
                              data_size=len(str(data)))
```

### 2. æ¡ä»¶æ—¥å¿—è®°å½•

```python
class ConditionalLoggingService(BaseService):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._enable_detailed_logging = os.getenv("DETAILED_LOGGING", "false").lower() == "true"

    def complex_operation(self, data: Dict):
        self._log_info("å¼€å§‹å¤æ‚æ“ä½œ")

        # è¯¦ç»†æ—¥å¿—ï¼ˆä»…åœ¨ç”Ÿäº§ç¯å¢ƒæˆ–è°ƒè¯•æ—¶å¯ç”¨ï¼‰
        if self._enable_detailed_logging:
            self._log_debug("è¯¦ç»†æ“ä½œæ­¥éª¤",
                           step=1,
                           data_preview=str(data)[:100])

        try:
            result = self._perform_operation(data)

            if self._enable_detailed_logging:
                self._log_debug("æ“ä½œç»“æœè¯¦æƒ…",
                               result_size=len(str(result)),
                               processing_time_ms=... )

            self._log_info("å¤æ‚æ“ä½œå®Œæˆ")
            return result

        except Exception as e:
            self._log_error("å¤æ‚æ“ä½œå¤±è´¥", error=e)
            raise
```

### 3. æ—¥å¿—èšåˆå’Œåˆ†æ

```python
class LogAggregator:
    def __init__(self):
        self.operation_counts = {}
        self.error_counts = {}
        self.performance_data = {}

    def aggregate_log_entry(self, log_entry: Dict):
        """èšåˆæ—¥å¿—æ¡ç›®"""
        level = log_entry.get("level")
        operation = log_entry.get("operation")

        # ç»Ÿè®¡æ“ä½œæ¬¡æ•°
        if operation:
            self.operation_counts[operation] = self.operation_counts.get(operation, 0) + 1

        # ç»Ÿè®¡é”™è¯¯æ¬¡æ•°
        if level == "ERROR":
            error_type = log_entry.get("extra_data", {}).get("error_type", "Unknown")
            self.error_counts[error_type] = self.error_counts.get(error_type, 0) + 1

        # èšåˆæ€§èƒ½æ•°æ®
        if "duration_ms" in log_entry:
            if operation not in self.performance_data:
                self.performance_data[operation] = []
            self.performance_data[operation].append(log_entry["duration_ms"])

    def get_performance_summary(self, operation: str) -> Dict:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        if operation not in self.performance_data:
            return {}

        durations = self.performance_data[operation]
        return {
            "operation": operation,
            "count": len(durations),
            "avg_ms": sum(durations) / len(durations),
            "min_ms": min(durations),
            "max_ms": max(durations),
            "p95_ms": sorted(durations)[int(len(durations) * 0.95)]
        }
```

## ğŸ› ï¸ æ—¥å¿—ç®¡ç†

### 1. æ—¥å¿—è½®è½¬é…ç½®

```python
# logging_config.py ä¸­çš„é…ç½®
class ServiceLoggerConfig:
    def __init__(self):
        # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        self.max_log_size = int(os.getenv("SERVICE_LOG_MAX_SIZE", "10")) * 1024 * 1024
        self.backup_count = int(os.getenv("SERVICE_LOG_BACKUP_COUNT", "5"))
        self.log_file_path = os.getenv("SERVICE_LOG_FILE_PATH", "logs/services.log")

        # åˆ›å»ºæ—¥å¿—ç›®å½•
        if self.enable_file:
            log_dir = Path(self.log_file_path).parent
            log_dir.mkdir(parents=True, exist_ok=True)
```

### 2. æ—¥å¿—æ–‡ä»¶ç®¡ç†

```bash
# æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶
ls -la logs/

# æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶å¤§å°
du -h logs/services.log*

# æ‰‹åŠ¨è½®è½¬æ—¥å¿—ï¼ˆå¦‚æœéœ€è¦ï¼‰
logrotate -f /etc/logrotate.d/tatake-services

# æ¸…ç†æ—§æ—¥å¿—
find logs/ -name "*.log.*" -mtime +30 -delete
```

### 3. æ—¥å¿—ç›‘æ§è„šæœ¬

```python
#!/usr/bin/env python3
"""
æ—¥å¿—ç›‘æ§è„šæœ¬
ç›‘æ§é”™è¯¯æ—¥å¿—å¹¶å‘é€å‘Šè­¦
"""

import re
import json
import time
import smtplib
from datetime import datetime
from email.mime.text import MimeText

class LogMonitor:
    def __init__(self, log_file: str):
        self.log_file = log_file
        self.error_patterns = [
            r'level":"ERROR"',
            r'level":"CRITICAL"',
            r'exception":'
        ]

    def monitor_errors(self, check_interval: int = 60):
        """ç›‘æ§é”™è¯¯æ—¥å¿—"""
        error_count = 0
        last_position = 0

        while True:
            try:
                with open(self.log_file, 'r') as f:
                    f.seek(last_position)
                    new_lines = f.readlines()
                    last_position = f.tell()

                    for line in new_lines:
                        if any(pattern in line for pattern in self.error_patterns):
                            error_count += 1
                            log_entry = json.loads(line)
                            self.handle_error(log_entry)

                            # å¦‚æœé”™è¯¯è¿‡å¤šï¼Œå‘é€å‘Šè­¦
                            if error_count >= 10:
                                self.send_alert("é”™è¯¯æ—¥å¿—è¿‡å¤šå‘Šè­¦", f"åœ¨è¿‡å»{check_interval}ç§’å†…å‘ç°{error_count}ä¸ªé”™è¯¯")
                                error_count = 0

                time.sleep(check_interval)

            except Exception as e:
                print(f"æ—¥å¿—ç›‘æ§å¼‚å¸¸: {e}")
                time.sleep(check_interval)

    def handle_error(self, log_entry: Dict):
        """å¤„ç†å•ä¸ªé”™è¯¯æ—¥å¿—"""
        timestamp = log_entry.get("timestamp")
        level = log_entry.get("level")
        service = log_entry.get("service")
        message = log_entry.get("message")

        print(f"[{timestamp}] {level} - {service}: {message}")

        # å¯ä»¥æ·»åŠ æ›´å¤šçš„é”™è¯¯å¤„ç†é€»è¾‘
        # ä¾‹å¦‚ï¼šå†™å…¥é”™è¯¯æ•°æ®åº“ã€å‘é€åˆ°ç›‘æ§ç³»ç»Ÿç­‰

    def send_alert(self, subject: str, message: str):
        """å‘é€å‘Šè­¦é‚®ä»¶"""
        # å®ç°é‚®ä»¶å‘é€é€»è¾‘
        pass

if __name__ == "__main__":
    monitor = LogMonitor("logs/services.log")
    monitor.monitor_errors()
```

## ğŸ“Š æ—¥å¿—åˆ†æ

### 1. é”™è¯¯æ—¥å¿—åˆ†æ

```bash
# ç»Ÿè®¡é”™è¯¯ç±»å‹
grep "level\":\"ERROR" logs/services.log | jq -r '.message' | sort | uniq -c | sort -nr

# æŸ¥çœ‹æœ€è¿‘çš„é”™è¯¯
tail -100 logs/services.log | grep "level\":\"ERROR" | jq

# åˆ†æé”™è¯¯è¶‹åŠ¿
grep "level\":\"ERROR" logs/services.log | jq -r '.timestamp' | cut -c1-13 | sort | uniq -c
```

### 2. æ€§èƒ½æ—¥å¿—åˆ†æ

```bash
# æŸ¥çœ‹æœ€æ…¢çš„æ“ä½œ
grep "duration_ms" logs/services.log | jq -c 'select(.duration_ms > 1000)' | sort -r

# ç»Ÿè®¡æ“ä½œæ€§èƒ½
grep "operation" logs/services.log | jq -r 'select(.duration_ms) | "\(.operation):\(.duration_ms)"' | \
awk -F: '{sum[$1] += $2; count[$1] += 1} END {for (op in sum) print op, sum[op]/count[op], count[op]}' | \
sort -k2 -nr
```

### 3. ç”¨æˆ·è¡Œä¸ºåˆ†æ

```bash
# åˆ†æç”¨æˆ·æ“ä½œé¢‘ç‡
grep "user_id" logs/services.log | jq -r '.user_id' | sort | uniq -c | sort -nr | head -10

# åˆ†ææ“ä½œç±»å‹åˆ†å¸ƒ
grep "operation" logs/services.log | jq -r '.operation' | sort | uniq -c | sort -nr
```

## ğŸ”§ æ•…éšœæ’é™¤

### 1. å¸¸è§æ—¥å¿—é—®é¢˜

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| æ—¥å¿—æ–‡ä»¶è¿‡å¤§ | è½®è½¬é…ç½®ä¸å½“ | è°ƒæ•´ `SERVICE_LOG_MAX_SIZE` å’Œ `SERVICE_LOG_BACKUP_COUNT` |
| æ—¥å¿—ä¸æ˜¾ç¤º | æ—¥å¿—çº§åˆ«è®¾ç½®è¿‡é«˜ | é™ä½ `SERVICE_LOG_LEVEL` åˆ° DEBUG æˆ– INFO |
| æ€§èƒ½å½±å“ | è¯¦ç»†æ—¥å¿—è¿‡å¤š | å…³é—­ `SERVICE_LOG_PERFORMANCE` æˆ–æé«˜æ—¥å¿—çº§åˆ« |
| ç£ç›˜ç©ºé—´ä¸è¶³ | æ—¥å¿—æ–‡ä»¶è¿‡å¤š | å¢åŠ æ—¥å¿—æ¸…ç†è„šæœ¬ï¼Œè°ƒæ•´å¤‡ä»½é…ç½® |

### 2. è°ƒè¯•æŠ€å·§

```python
# ä¸´æ—¶å¯ç”¨è¯¦ç»†æ—¥å¿—
import os
os.environ['SERVICE_LOG_LEVEL'] = 'DEBUG'
os.environ['SERVICE_LOG_PERFORMANCE'] = 'true'

# é‡æ–°åˆ›å»ºæœåŠ¡å®ä¾‹
service = TaskService()

# ç°åœ¨ä¼šçœ‹åˆ°è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—
```

### 3. æ—¥å¿—éªŒè¯

```python
def test_logging_system():
    """æµ‹è¯•æ—¥å¿—ç³»ç»Ÿæ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    from src.services.logging_config import get_logger

    logger = get_logger("TestService")

    # æµ‹è¯•å„çº§åˆ«æ—¥å¿—
    logger.debug("æµ‹è¯•DEBUGæ—¥å¿—")
    logger.info("æµ‹è¯•INFOæ—¥å¿—")
    logger.warning("æµ‹è¯•WARNINGæ—¥å¿—")
    logger.error("æµ‹è¯•ERRORæ—¥å¿—")

    # æµ‹è¯•æ“ä½œæ—¥å¿—
    logger.log_operation_start("æµ‹è¯•æ“ä½œ")
    logger.log_operation_success("æµ‹è¯•æ“ä½œ", duration_ms=100)

    print("æ—¥å¿—ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼Œè¯·æ£€æŸ¥æ—¥å¿—è¾“å‡º")
```

## ğŸ“ˆ æ€§èƒ½è€ƒè™‘

### 1. æ—¥å¿—æ€§èƒ½ä¼˜åŒ–

```python
# ä½¿ç”¨å»¶è¿Ÿå­—ç¬¦ä¸²æ ¼å¼åŒ–
def optimized_logging(self, large_data: Dict):
    # âŒ ä¸æ¨èï¼šç«‹å³æ ¼å¼åŒ–å¤§å­—ç¬¦ä¸²
    # self._log_debug(f"å¤§æ•°æ®: {json.dumps(large_data)}")

    # âœ… æ¨èï¼šåªåœ¨éœ€è¦æ—¶æ ¼å¼åŒ–
    if self._logger.isEnabledFor(logging.DEBUG):
        self._log_debug("å¤§æ•°æ®", data_preview=str(large_data)[:100])

# é¿å…åœ¨çƒ­è·¯å¾„ä¸­è®°å½•è¯¦ç»†æ—¥å¿—
def hot_path_operation(self):
    # âŒ ä¸æ¨èï¼šåœ¨å¾ªç¯ä¸­è®°å½•æ—¥å¿—
    # for item in large_list:
    #     self._log_debug("å¤„ç†é¡¹ç›®", item=item)

    # âœ… æ¨èï¼šè®°å½•æ±‡æ€»ä¿¡æ¯
    self._log_info("å¼€å§‹æ‰¹é‡å¤„ç†", total_items=len(large_list))

    for item in large_list:
        self._process_item(item)

    self._log_info("æ‰¹é‡å¤„ç†å®Œæˆ")
```

### 2. å¼‚æ­¥æ—¥å¿—è®°å½•

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncLogger:
    def __init__(self, logger):
        self.logger = logger
        self.executor = ThreadPoolExecutor(max_workers=2)

    async def log_async(self, level: str, message: str, **kwargs):
        """å¼‚æ­¥è®°å½•æ—¥å¿—"""
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            self.executor,
            getattr(self.logger, level.lower()),
            message,
            **kwargs
        )

    def close(self):
        """å…³é—­æ—¥å¿—å™¨"""
        self.executor.shutdown(wait=True)
```

## ğŸ¯ æœ€ä½³å®è·µæ€»ç»“

### 1. æ—¥å¿—çº§åˆ«ä½¿ç”¨æŒ‡å—

- **DEBUG**: è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ï¼Œä»…åœ¨å¼€å‘æ—¶ä½¿ç”¨
- **INFO**: ä¸€èˆ¬ä¿¡æ¯ï¼Œè®°å½•é‡è¦çš„ä¸šåŠ¡æ“ä½œ
- **WARNING**: è­¦å‘Šä¿¡æ¯ï¼Œæ½œåœ¨çš„é—®é¢˜
- **ERROR**: é”™è¯¯ä¿¡æ¯ï¼Œæ“ä½œå¤±è´¥ä½†ç³»ç»Ÿå¯ç»§ç»­è¿è¡Œ
- **CRITICAL**: ä¸¥é‡é”™è¯¯ï¼Œå¯èƒ½å½±å“ç³»ç»Ÿæ­£å¸¸è¿è¡Œ

### 2. æ—¥å¿—å†…å®¹åŸåˆ™

- **ç»“æ„åŒ–**: ä½¿ç”¨JSONæ ¼å¼ï¼Œä¾¿äºè§£æå’Œåˆ†æ
- **ä¸Šä¸‹æ–‡**: åŒ…å«è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆç”¨æˆ·IDã€æ“ä½œIDç­‰ï¼‰
- **ç®€æ´**: é¿å…å†—ä½™ä¿¡æ¯ï¼Œé‡ç‚¹çªå‡º
- **ä¸€è‡´**: ä½¿ç”¨ç»Ÿä¸€çš„å­—æ®µå‘½åå’Œæ ¼å¼

### 3. æ€§èƒ½è€ƒè™‘

- **é€‚åº¦è®°å½•**: é¿å…è¿‡åº¦è®°å½•å½±å“æ€§èƒ½
- **å¼‚æ­¥å¤„ç†**: è€ƒè™‘å¼‚æ­¥æ—¥å¿—è®°å½•
- **æ¡ä»¶æ—¥å¿—**: æ ¹æ®ç¯å¢ƒæˆ–é…ç½®é€‰æ‹©æ€§è®°å½•
- **å®šæœŸæ¸…ç†**: åŠæ—¶æ¸…ç†å’Œå½’æ¡£å†å²æ—¥å¿—

---

**æœ€åæ›´æ–°**: 2025-10-20
**ç‰ˆæœ¬**: 1.0.0
**ç»´æŠ¤è€…**: å¼€å‘å›¢é˜Ÿ