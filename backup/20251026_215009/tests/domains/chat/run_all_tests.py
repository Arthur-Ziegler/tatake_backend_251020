"""
èŠå¤©å·¥å…·æµ‹è¯•è¿è¡Œå™¨

è¿è¡Œæ‰€æœ‰èŠå¤©å·¥å…·æµ‹è¯•å¥—ä»¶ï¼ŒåŒ…æ‹¬ï¼š
1. åŸºç¡€å·¥å…·æµ‹è¯•
2. CRUDå·¥å…·æµ‹è¯•
3. æœç´¢å·¥å…·æµ‹è¯•
4. æ‰¹é‡å·¥å…·æµ‹è¯•
5. é›†æˆæµ‹è¯•

ä½¿ç”¨æ–¹æ³•ï¼š
python -m tests.domains.chat.run_all_tests

æˆ–è€…ï¼š
uv run python -m tests.domains.chat.run_all_tests

ä½œè€…ï¼šTaKeKeå›¢é˜Ÿ
ç‰ˆæœ¬ï¼š1.0.0
"""

import sys
import os
import logging
import traceback
from datetime import datetime
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
sys.path.insert(0, project_root)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('chat_tools_test_results.log', encoding='utf-8')
    ]
)

logger = logging.getLogger(__name__)


class TestRunner:
    """æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self):
        self.test_results = []
        self.start_time = datetime.now()

    def run_test_module(self, module_name: str, test_class_name: str) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•æ¨¡å—"""
        logger.info(f"ğŸ”„ å¼€å§‹è¿è¡Œæµ‹è¯•æ¨¡å—: {module_name}.{test_class_name}")

        try:
            # åŠ¨æ€å¯¼å…¥æµ‹è¯•æ¨¡å—
            module = __import__(module_name, fromlist=[test_class_name])
            test_class = getattr(module, test_class_name)

            # å®ä¾‹åŒ–æµ‹è¯•ç±»
            test_instance = test_class()

            # è·å–æ‰€æœ‰æµ‹è¯•æ–¹æ³•
            test_methods = [
                method for method in dir(test_instance)
                if method.startswith('test_') and callable(getattr(test_instance, method))
            ]

            passed_tests = 0
            failed_tests = []
            total_tests = len(test_methods)

            # è¿è¡Œæ¯ä¸ªæµ‹è¯•æ–¹æ³•
            for method_name in test_methods:
                try:
                    method = getattr(test_instance, method_name)
                    method()
                    passed_tests += 1
                    logger.info(f"âœ… {method_name} é€šè¿‡")

                except Exception as e:
                    failed_tests.append({
                        'method': method_name,
                        'error': str(e),
                        'traceback': traceback.format_exc()
                    })
                    logger.error(f"âŒ {method_name} å¤±è´¥: {e}")

            # è®¡ç®—ç»“æœ
            success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

            result = {
                'module': module_name,
                'class': test_class_name,
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'failed_tests': len(failed_tests),
                'success_rate': success_rate,
                'failed_details': failed_tests,
                'status': 'PASSED' if len(failed_tests) == 0 else 'FAILED'
            }

            logger.info(f"ğŸ“Š {module_name} ç»“æœ: {passed_tests}/{total_tests} é€šè¿‡ ({success_rate:.1f}%)")

            return result

        except Exception as e:
            logger.error(f"ğŸ’¥ æµ‹è¯•æ¨¡å— {module_name} è¿è¡Œå¤±è´¥: {e}")
            return {
                'module': module_name,
                'class': test_class_name,
                'total_tests': 0,
                'passed_tests': 0,
                'failed_tests': 1,
                'success_rate': 0.0,
                'failed_details': [{'error': str(e), 'traceback': traceback.format_exc()}],
                'status': 'CRASHED'
            }

    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹è¿è¡ŒèŠå¤©å·¥å…·å®Œæ•´æµ‹è¯•å¥—ä»¶")

        # æµ‹è¯•æ¨¡å—é…ç½®
        test_modules = [
            ('tests.domains.chat.test_chat_tools_basic', 'TestBasicTools'),
            ('tests.domains.chat.test_task_crud', 'TestTaskCrudTools'),
            ('tests.domains.chat.test_task_search', 'TestSearchTasks'),
            ('tests.domains.chat.test_task_batch', 'TestBatchCreateSubtasks'),
            ('tests.domains.chat.test_chat_tools_integration', 'TestCompleteWorkflow'),
            ('tests.domains.chat.test_chat_tools_integration', 'TestToolChaining'),
            ('tests.domains.chat.test_chat_tools_integration', 'TestErrorHandlingAndRecovery'),
            ('tests.domains.chat.test_chat_tools_integration', 'TestPerformanceIntegration'),
            ('tests.domains.chat.test_chat_tools_integration', 'TestRealWorldScenarios'),
            ('tests.domains.chat.test_chat_tools_integration', 'TestLangGraphIntegration')
        ]

        # è¿è¡Œæ‰€æœ‰æµ‹è¯•æ¨¡å—
        for module_name, class_name in test_modules:
            result = self.run_test_module(module_name, class_name)
            self.test_results.append(result)

            # å¦‚æœæœ‰æµ‹è¯•å¤±è´¥ï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯
            if result['failed_tests'] > 0:
                logger.error(f"âŒ {module_name} å¤±è´¥è¯¦æƒ…:")
                for failed in result['failed_details']:
                    method_name = failed.get('method', 'æœªçŸ¥æ–¹æ³•')
                    error_msg = failed.get('error', 'æœªçŸ¥é”™è¯¯')
                    logger.error(f"   - {method_name}: {error_msg}")

        # ç”Ÿæˆæ€»ä½“æŠ¥å‘Š
        return self.generate_summary_report()

    def generate_summary_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ€»ç»“æŠ¥å‘Š"""
        total_modules = len(self.test_results)
        total_tests = sum(result['total_tests'] for result in self.test_results)
        total_passed = sum(result['passed_tests'] for result in self.test_results)
        total_failed = sum(result['failed_tests'] for result in self.test_results)
        overall_success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0

        passed_modules = sum(1 for result in self.test_results if result['status'] == 'PASSED')
        failed_modules = sum(1 for result in self.test_results if result['status'] == 'FAILED')
        crashed_modules = sum(1 for result in self.test_results if result['status'] == 'CRASHED')

        execution_time = datetime.now() - self.start_time

        summary = {
            'execution_info': {
                'start_time': self.start_time.isoformat(),
                'execution_time_seconds': execution_time.total_seconds(),
                'execution_time_formatted': str(execution_time).split('.')[0]
            },
            'module_summary': {
                'total_modules': total_modules,
                'passed_modules': passed_modules,
                'failed_modules': failed_modules,
                'crashed_modules': crashed_modules
            },
            'test_summary': {
                'total_tests': total_tests,
                'total_passed': total_passed,
                'total_failed': total_failed,
                'overall_success_rate': overall_success_rate
            },
            'detailed_results': self.test_results,
            'overall_status': 'PASSED' if total_failed == 0 else 'FAILED'
        }

        return summary

    def print_summary_report(self, summary: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ€»ç»“æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ¯ èŠå¤©å·¥å…·æµ‹è¯•å¥—ä»¶æ€»ç»“æŠ¥å‘Š")
        print("="*80)

        # æ‰§è¡Œä¿¡æ¯
        exec_info = summary['execution_info']
        print(f"â° æ‰§è¡Œæ—¶é—´: {exec_info['execution_time_formatted']}")
        print(f"ğŸš€ å¼€å§‹æ—¶é—´: {exec_info['start_time'][:19]}")

        # æ¨¡å—ç»Ÿè®¡
        module_summary = summary['module_summary']
        print(f"\nğŸ“‹ æ¨¡å—ç»Ÿè®¡:")
        print(f"   æ€»æ¨¡å—æ•°: {module_summary['total_modules']}")
        print(f"   é€šè¿‡æ¨¡å—: {module_summary['passed_modules']} âœ…")
        print(f"   å¤±è´¥æ¨¡å—: {module_summary['failed_modules']} âŒ")
        print(f"   å´©æºƒæ¨¡å—: {module_summary['crashed_modules']} ğŸ’¥")

        # æµ‹è¯•ç»Ÿè®¡
        test_summary = summary['test_summary']
        print(f"\nğŸ§ª æµ‹è¯•ç»Ÿè®¡:")
        print(f"   æ€»æµ‹è¯•æ•°: {test_summary['total_tests']}")
        print(f"   é€šè¿‡æµ‹è¯•: {test_summary['total_passed']} âœ…")
        print(f"   å¤±è´¥æµ‹è¯•: {test_summary['total_failed']} âŒ")
        print(f"   æˆåŠŸç‡: {test_summary['overall_success_rate']:.1f}%")

        # è¯¦ç»†ç»“æœ
        print(f"\nğŸ“Š è¯¦ç»†ç»“æœ:")
        for result in summary['detailed_results']:
            status_emoji = "âœ…" if result['status'] == 'PASSED' else "âŒ"
            print(f"   {status_emoji} {result['module']}: "
                  f"{result['passed_tests']}/{result['total_tests']} "
                  f"({result['success_rate']:.1f}%)")

        # æ•´ä½“çŠ¶æ€
        overall_status = summary['overall_status']
        status_emoji = "ğŸ‰" if overall_status == 'PASSED' else "âŒ"
        status_text = "æ‰€æœ‰æµ‹è¯•é€šè¿‡" if overall_status == 'PASSED' else "å­˜åœ¨æµ‹è¯•å¤±è´¥"

        print(f"\n{status_emoji} æ•´ä½“çŠ¶æ€: {status_text}")
        print("="*80)

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = "chat_tools_test_report.json"
        try:
            import json
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    runner = TestRunner()

    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        summary = runner.run_all_tests()

        # æ‰“å°æ€»ç»“æŠ¥å‘Š
        runner.print_summary_report(summary)

        # è¿”å›é€‚å½“çš„é€€å‡ºç 
        if summary['overall_status'] == 'PASSED':
            logger.info("ğŸ‰ æ‰€æœ‰èŠå¤©å·¥å…·æµ‹è¯•é€šè¿‡ï¼")
            return 0
        else:
            logger.error("âŒ å­˜åœ¨æµ‹è¯•å¤±è´¥ï¼")
            return 1

    except Exception as e:
        logger.error(f"ğŸ’¥ æµ‹è¯•è¿è¡Œå™¨å´©æºƒ: {e}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)